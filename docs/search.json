[
  {
    "objectID": "blog/project4/hw4_questions (1).html",
    "href": "blog/project4/hw4_questions (1).html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "In this section, we implement the K-Means clustering algorithm from scratch and visualize the iterative steps it takes to converge. K-Means is an unsupervised learning algorithm that groups data into k clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids.\nTo demonstrate this, we use the Palmer Penguins dataset, focusing on two continuous numerical features: bill length (mm) and flipper length (mm). These two variables are biologically meaningful and expected to produce distinguishable clusters based on species differences.\nAfter running our custom implementation, we visualize the clustering result and compare it to the output of Python’s built-in KMeans function to assess consistency and accuracy.\n\n\nWe begin by loading the Palmer Penguins dataset and selecting the two numerical variables: bill length and flipper length. We standardize them to ensure fair distance calculations.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Load and clean\n\npenguins = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project4/palmer_penguins.csv\")\nX = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# Standardize\nX_scaled = StandardScaler().fit_transform(X)\nprint(\"Data loaded and standardized. First 5 rows:\")\nprint(X_scaled[:5])\n\nData loaded and standardized. First 5 rows:\n[[-0.89604189 -1.42675157]\n [-0.82278787 -1.06947358]\n [-0.67627982 -0.42637319]\n [-1.33556603 -0.56928439]\n [-0.85941488 -0.78365118]]\n\n\n\n\n\nWe randomly select k data points as the initial centroids. This step is critical, as poor initialization can affect clustering quality.\n\n\nInitial Centroid Coordinates:\n[[-1.17074448 -1.14092917]\n [-1.51870109 -1.14092917]\n [ 1.41145984 -0.49782879]]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, the initial centroids (marked by red Xs) are scattered across distinct regions of the data cloud, which suggests a good starting point for the K-Means algorithm. Their placement roughly aligns with visible groupings in the standardized feature space of bill length and flipper length. This helps ensure that the algorithm can quickly begin assigning points to meaningful clusters, potentially leading to faster convergence and more accurate segmentation. However, it’s important to note that random initialization can sometimes result in suboptimal clustering if centroids are poorly positioned, which is why techniques like k-means++ are often used in practice.\n\n\n\nEach point is assigned to the closest centroid using Euclidean distance. This defines the cluster membership for the current iteration.\n\n\n Cluster assignments for first 10 points:\n[0 0 0 0 0 0 0 0 0 1]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, each data point has been assigned to the nearest initial centroid based on Euclidean distance, resulting in three distinct color-coded clusters. Despite using only the very first randomly selected centroids (shown as black Xs), the cluster boundaries already begin to reflect meaningful groupings in the data. Notably, the cluster labeled “2” (in purple) aligns with the upper-right region of the plot, while clusters “0” and “1” (green and orange) split the more densely packed lower-left region. This early clustering step sets the stage for iterative refinement, where centroids will shift and reassignments will occur to minimize within-cluster variation. The clear separation in certain regions suggests that the initial centroid placement was reasonably effective for kickstarting the K-Means process.\n\n\n\nCentroids are recalculated by averaging all points assigned to each cluster.\n\n\n Updated Centroid Coordinates After One Iteration:\n[[-0.7603627  -0.72772941]\n [-1.56945566 -0.88981379]\n [ 0.77799267  0.62749926]]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, the centroids have shifted significantly after the first update step in the K-Means algorithm. The blue X markers represent the new centroid positions, calculated as the mean of the points assigned to each cluster during the initial assignment. These updated centroids now lie closer to the centers of their respective data groupings, reflecting the algorithm’s attempt to better capture the natural structure of the dataset. This shift highlights how K-Means iteratively improves cluster accuracy by adjusting centroids to minimize within-cluster variation. The movement from the initial (black) to updated (blue) centroids is a key part of the convergence process that ultimately leads to stable and meaningful clustering.\n\n\n\nNow we combine the steps into a loop to run the full K-Means algorithm over multiple iterations until convergence.\n\ndef custom_kmeans(X, k=3, max_iters=10, seed=0):\n    centroids = initialize_centroids(X, k, seed)\n    history = []\n\n    for i in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        history.append((centroids.copy(), labels.copy()))\n        new_centroids = update_centroids(X, labels, k)\n\n        print(f\"Iteration {i+1}\")\n        print(\"Centroids:\")\n        print(new_centroids)\n        print()\n\n        if np.allclose(new_centroids, centroids):\n            print(\"Converged.\")\n            break\n        centroids = new_centroids\n\n    return centroids, labels, history\n\nfinal_centroids, final_labels, history = custom_kmeans(X_scaled, k=3)\n\nIteration 1\nCentroids:\n[[-0.7603627  -0.72772941]\n [-1.56945566 -0.88981379]\n [ 0.77799267  0.62749926]]\n\nIteration 2\nCentroids:\n[[-0.53323649 -0.70962061]\n [-1.45566036 -0.98839896]\n [ 0.79343288  0.76567431]]\n\nIteration 3\nCentroids:\n[[-0.38273033 -0.64837117]\n [-1.35738065 -0.97910326]\n [ 0.8131056   0.82321761]]\n\nIteration 4\nCentroids:\n[[-0.23449222 -0.6062165 ]\n [-1.26705197 -0.96018854]\n [ 0.80861148  0.85263707]]\n\nIteration 5\nCentroids:\n[[-0.08274572 -0.57961532]\n [-1.21690977 -0.93326134]\n [ 0.80318982  0.89416338]]\n\nIteration 6\nCentroids:\n[[ 0.07754986 -0.53534298]\n [-1.1710933  -0.92724291]\n [ 0.78892438  0.94721584]]\n\nIteration 7\nCentroids:\n[[ 0.25288963 -0.48466592]\n [-1.11962591 -0.91351397]\n [ 0.77138991  0.99921631]]\n\nIteration 8\nCentroids:\n[[ 0.42918998 -0.43750913]\n [-1.08593193 -0.89552019]\n [ 0.7420602   1.06672886]]\n\nIteration 9\nCentroids:\n[[ 0.59322179 -0.38056832]\n [-1.05399588 -0.88469387]\n [ 0.69795412  1.12539483]]\n\nIteration 10\nCentroids:\n[[ 0.75243157 -0.3911486 ]\n [-1.01561831 -0.84617483]\n [ 0.67223372  1.1337407 ]]\n\n\n\nAs we can see from this section, the full iterative K-Means process refines the cluster centroids over multiple iterations, gradually improving their positions. The printed output on the right shows how the centroid coordinates evolve from iteration to iteration. Initially, the centroids move significantly, adjusting based on the new cluster assignments. By the third or fourth iteration, the changes become smaller, indicating that the centroids are stabilizing. This behavior reflects the algorithm’s convergence toward a local minimum, where subsequent updates no longer result in substantial shifts. The convergence condition used here — checking whether the new centroids are sufficiently close to the previous ones — ensures that the process stops when further improvements are negligible. Overall, this iterative loop effectively captures the core mechanics of K-Means and demonstrates how simple updates can lead to increasingly meaningful clustering.\n\n\n\nWe plot how clusters evolve over the first 4 iterations.\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i in range(4):\n    centroids, labels = history[i]\n    ax = axes[i]\n    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels, palette='Set2', s=50, ax=ax, legend=False)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=150, marker='X')\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length\")\n    ax.set_ylabel(\"Flipper Length\")\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see from the four subplots, the K-Means clustering algorithm rapidly refines its cluster assignments and centroid positions during the initial iterations. In Iteration 1, the centroids (black Xs) are still close to their random starting points, and the cluster assignments reflect some overlap and misalignment. By Iteration 2, we observe a noticeable repositioning of centroids, and many data points have shifted to more appropriate clusters. The transition continues in Iterations 3 and 4, where clusters become more distinct and compact, and centroids migrate toward the true centers of the data groupings. This sequence of visualizations clearly demonstrates the power of iterative refinement in K-Means: even after just a few steps, the algorithm can uncover meaningful structure from initially random assumptions.\n\n\n\nWe now compare our implementation with scikit-learn’s built-in KMeans.\n\n\n\n\n\n\n\n\n\nAs we can see from the side-by-side plots, both the custom K-Means implementation (left) and the scikit-learn version (right) identify similar underlying cluster structures in the dataset. Each cluster in both plots is color-coded and labeled, with centroids marked as Xs — black for the custom model and red for scikit-learn.\nWhile the exact label assignments may differ due to random initialization and the lack of enforced label matching between implementations, the overall groupings are visually consistent. Both methods successfully separate the data into three coherent clusters based on the standardized bill length and flipper length of the penguins.\nThe consistency in centroid positions and cluster boundaries confirms that the custom algorithm behaves as expected and is a reliable reimplementation of the K-Means clustering approach.\n\n\n\nTo quantify how similar our clustering is to the built-in one:\n\nfrom sklearn.metrics import adjusted_rand_score\n\n# Compute Adjusted Rand Index (ARI)\nari = adjusted_rand_score(sk_labels, final_labels)\n\nprint(f\"Adjusted Rand Index between custom and sklearn KMeans: {ari:.4f}\")\n\nAdjusted Rand Index between custom and sklearn KMeans: 0.9048\n\n\nThe Adjusted Rand Index (ARI) score of 0.9048 indicates a very strong agreement between the cluster assignments generated by the custom K-Means implementation and those produced by scikit-learn’s built-in version. Since the ARI accounts for chance grouping, a value this close to 1.0 suggests that both methods identified nearly identical structure in the data. This confirms that the custom implementation is not only functional but closely replicates the behavior of a well-established library method.\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the Within-Cluster Sum of Squares (WCSS), the “elbow” appears at K = 3, suggesting that three clusters offer the best trade-off between compression and simplicity.\nThe Silhouette Score is highest at K = 2, but only slightly higher than at K = 3. Since silhouette values remain reasonably strong for K=3 while dropping significantly afterward, K=3 still represents a robust choice that balances separation and interpretability.\nConclusion: While K=2 has the highest silhouette score, K=3 is the best overall choice as supported by both the elbow method and the silhouette curve — and it aligns well with domain knowledge about penguin species grouping.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imageio\n\nfilenames = []\n\nfor i, (centroids, labels) in enumerate(history):\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels, palette='Set2', s=50, ax=ax, legend=False)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=150, marker='X')\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length\")\n    ax.set_ylabel(\"Flipper Length\")\n    ax.grid(True)\n\n    filename = f\"kmeans_iter_{i}.png\"\n    filenames.append(filename)\n    plt.savefig(filename)\n    plt.close()\n\n# Create animated GIF\nwith imageio.get_writer(\"kmeans_animation.gif\", mode=\"I\", duration=0.8) as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\nprint(\"GIF saved as kmeans_animation.gif\")\n\nGIF saved as kmeans_animation.gif"
  },
  {
    "objectID": "blog/project4/hw4_questions (1).html#a.-k-means",
    "href": "blog/project4/hw4_questions (1).html#a.-k-means",
    "title": "K-Means Clustering",
    "section": "",
    "text": "In this section, we implement the K-Means clustering algorithm from scratch and visualize the iterative steps it takes to converge. K-Means is an unsupervised learning algorithm that groups data into k clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids.\nTo demonstrate this, we use the Palmer Penguins dataset, focusing on two continuous numerical features: bill length (mm) and flipper length (mm). These two variables are biologically meaningful and expected to produce distinguishable clusters based on species differences.\nAfter running our custom implementation, we visualize the clustering result and compare it to the output of Python’s built-in KMeans function to assess consistency and accuracy.\n\n\nWe begin by loading the Palmer Penguins dataset and selecting the two numerical variables: bill length and flipper length. We standardize them to ensure fair distance calculations.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Load and clean\n\npenguins = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project4/palmer_penguins.csv\")\nX = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n# Standardize\nX_scaled = StandardScaler().fit_transform(X)\nprint(\"Data loaded and standardized. First 5 rows:\")\nprint(X_scaled[:5])\n\nData loaded and standardized. First 5 rows:\n[[-0.89604189 -1.42675157]\n [-0.82278787 -1.06947358]\n [-0.67627982 -0.42637319]\n [-1.33556603 -0.56928439]\n [-0.85941488 -0.78365118]]\n\n\n\n\n\nWe randomly select k data points as the initial centroids. This step is critical, as poor initialization can affect clustering quality.\n\n\nInitial Centroid Coordinates:\n[[-1.17074448 -1.14092917]\n [-1.51870109 -1.14092917]\n [ 1.41145984 -0.49782879]]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, the initial centroids (marked by red Xs) are scattered across distinct regions of the data cloud, which suggests a good starting point for the K-Means algorithm. Their placement roughly aligns with visible groupings in the standardized feature space of bill length and flipper length. This helps ensure that the algorithm can quickly begin assigning points to meaningful clusters, potentially leading to faster convergence and more accurate segmentation. However, it’s important to note that random initialization can sometimes result in suboptimal clustering if centroids are poorly positioned, which is why techniques like k-means++ are often used in practice.\n\n\n\nEach point is assigned to the closest centroid using Euclidean distance. This defines the cluster membership for the current iteration.\n\n\n Cluster assignments for first 10 points:\n[0 0 0 0 0 0 0 0 0 1]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, each data point has been assigned to the nearest initial centroid based on Euclidean distance, resulting in three distinct color-coded clusters. Despite using only the very first randomly selected centroids (shown as black Xs), the cluster boundaries already begin to reflect meaningful groupings in the data. Notably, the cluster labeled “2” (in purple) aligns with the upper-right region of the plot, while clusters “0” and “1” (green and orange) split the more densely packed lower-left region. This early clustering step sets the stage for iterative refinement, where centroids will shift and reassignments will occur to minimize within-cluster variation. The clear separation in certain regions suggests that the initial centroid placement was reasonably effective for kickstarting the K-Means process.\n\n\n\nCentroids are recalculated by averaging all points assigned to each cluster.\n\n\n Updated Centroid Coordinates After One Iteration:\n[[-0.7603627  -0.72772941]\n [-1.56945566 -0.88981379]\n [ 0.77799267  0.62749926]]\n\n\n\n\n\n\n\n\n\nAs we can see from the graph, the centroids have shifted significantly after the first update step in the K-Means algorithm. The blue X markers represent the new centroid positions, calculated as the mean of the points assigned to each cluster during the initial assignment. These updated centroids now lie closer to the centers of their respective data groupings, reflecting the algorithm’s attempt to better capture the natural structure of the dataset. This shift highlights how K-Means iteratively improves cluster accuracy by adjusting centroids to minimize within-cluster variation. The movement from the initial (black) to updated (blue) centroids is a key part of the convergence process that ultimately leads to stable and meaningful clustering.\n\n\n\nNow we combine the steps into a loop to run the full K-Means algorithm over multiple iterations until convergence.\n\ndef custom_kmeans(X, k=3, max_iters=10, seed=0):\n    centroids = initialize_centroids(X, k, seed)\n    history = []\n\n    for i in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        history.append((centroids.copy(), labels.copy()))\n        new_centroids = update_centroids(X, labels, k)\n\n        print(f\"Iteration {i+1}\")\n        print(\"Centroids:\")\n        print(new_centroids)\n        print()\n\n        if np.allclose(new_centroids, centroids):\n            print(\"Converged.\")\n            break\n        centroids = new_centroids\n\n    return centroids, labels, history\n\nfinal_centroids, final_labels, history = custom_kmeans(X_scaled, k=3)\n\nIteration 1\nCentroids:\n[[-0.7603627  -0.72772941]\n [-1.56945566 -0.88981379]\n [ 0.77799267  0.62749926]]\n\nIteration 2\nCentroids:\n[[-0.53323649 -0.70962061]\n [-1.45566036 -0.98839896]\n [ 0.79343288  0.76567431]]\n\nIteration 3\nCentroids:\n[[-0.38273033 -0.64837117]\n [-1.35738065 -0.97910326]\n [ 0.8131056   0.82321761]]\n\nIteration 4\nCentroids:\n[[-0.23449222 -0.6062165 ]\n [-1.26705197 -0.96018854]\n [ 0.80861148  0.85263707]]\n\nIteration 5\nCentroids:\n[[-0.08274572 -0.57961532]\n [-1.21690977 -0.93326134]\n [ 0.80318982  0.89416338]]\n\nIteration 6\nCentroids:\n[[ 0.07754986 -0.53534298]\n [-1.1710933  -0.92724291]\n [ 0.78892438  0.94721584]]\n\nIteration 7\nCentroids:\n[[ 0.25288963 -0.48466592]\n [-1.11962591 -0.91351397]\n [ 0.77138991  0.99921631]]\n\nIteration 8\nCentroids:\n[[ 0.42918998 -0.43750913]\n [-1.08593193 -0.89552019]\n [ 0.7420602   1.06672886]]\n\nIteration 9\nCentroids:\n[[ 0.59322179 -0.38056832]\n [-1.05399588 -0.88469387]\n [ 0.69795412  1.12539483]]\n\nIteration 10\nCentroids:\n[[ 0.75243157 -0.3911486 ]\n [-1.01561831 -0.84617483]\n [ 0.67223372  1.1337407 ]]\n\n\n\nAs we can see from this section, the full iterative K-Means process refines the cluster centroids over multiple iterations, gradually improving their positions. The printed output on the right shows how the centroid coordinates evolve from iteration to iteration. Initially, the centroids move significantly, adjusting based on the new cluster assignments. By the third or fourth iteration, the changes become smaller, indicating that the centroids are stabilizing. This behavior reflects the algorithm’s convergence toward a local minimum, where subsequent updates no longer result in substantial shifts. The convergence condition used here — checking whether the new centroids are sufficiently close to the previous ones — ensures that the process stops when further improvements are negligible. Overall, this iterative loop effectively captures the core mechanics of K-Means and demonstrates how simple updates can lead to increasingly meaningful clustering.\n\n\n\nWe plot how clusters evolve over the first 4 iterations.\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i in range(4):\n    centroids, labels = history[i]\n    ax = axes[i]\n    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels, palette='Set2', s=50, ax=ax, legend=False)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=150, marker='X')\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length\")\n    ax.set_ylabel(\"Flipper Length\")\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see from the four subplots, the K-Means clustering algorithm rapidly refines its cluster assignments and centroid positions during the initial iterations. In Iteration 1, the centroids (black Xs) are still close to their random starting points, and the cluster assignments reflect some overlap and misalignment. By Iteration 2, we observe a noticeable repositioning of centroids, and many data points have shifted to more appropriate clusters. The transition continues in Iterations 3 and 4, where clusters become more distinct and compact, and centroids migrate toward the true centers of the data groupings. This sequence of visualizations clearly demonstrates the power of iterative refinement in K-Means: even after just a few steps, the algorithm can uncover meaningful structure from initially random assumptions.\n\n\n\nWe now compare our implementation with scikit-learn’s built-in KMeans.\n\n\n\n\n\n\n\n\n\nAs we can see from the side-by-side plots, both the custom K-Means implementation (left) and the scikit-learn version (right) identify similar underlying cluster structures in the dataset. Each cluster in both plots is color-coded and labeled, with centroids marked as Xs — black for the custom model and red for scikit-learn.\nWhile the exact label assignments may differ due to random initialization and the lack of enforced label matching between implementations, the overall groupings are visually consistent. Both methods successfully separate the data into three coherent clusters based on the standardized bill length and flipper length of the penguins.\nThe consistency in centroid positions and cluster boundaries confirms that the custom algorithm behaves as expected and is a reliable reimplementation of the K-Means clustering approach.\n\n\n\nTo quantify how similar our clustering is to the built-in one:\n\nfrom sklearn.metrics import adjusted_rand_score\n\n# Compute Adjusted Rand Index (ARI)\nari = adjusted_rand_score(sk_labels, final_labels)\n\nprint(f\"Adjusted Rand Index between custom and sklearn KMeans: {ari:.4f}\")\n\nAdjusted Rand Index between custom and sklearn KMeans: 0.9048\n\n\nThe Adjusted Rand Index (ARI) score of 0.9048 indicates a very strong agreement between the cluster assignments generated by the custom K-Means implementation and those produced by scikit-learn’s built-in version. Since the ARI accounts for chance grouping, a value this close to 1.0 suggests that both methods identified nearly identical structure in the data. This confirms that the custom implementation is not only functional but closely replicates the behavior of a well-established library method.\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the Within-Cluster Sum of Squares (WCSS), the “elbow” appears at K = 3, suggesting that three clusters offer the best trade-off between compression and simplicity.\nThe Silhouette Score is highest at K = 2, but only slightly higher than at K = 3. Since silhouette values remain reasonably strong for K=3 while dropping significantly afterward, K=3 still represents a robust choice that balances separation and interpretability.\nConclusion: While K=2 has the highest silhouette score, K=3 is the best overall choice as supported by both the elbow method and the silhouette curve — and it aligns well with domain knowledge about penguin species grouping.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imageio\n\nfilenames = []\n\nfor i, (centroids, labels) in enumerate(history):\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels, palette='Set2', s=50, ax=ax, legend=False)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=150, marker='X')\n    ax.set_title(f\"Iteration {i+1}\")\n    ax.set_xlabel(\"Bill Length\")\n    ax.set_ylabel(\"Flipper Length\")\n    ax.grid(True)\n\n    filename = f\"kmeans_iter_{i}.png\"\n    filenames.append(filename)\n    plt.savefig(filename)\n    plt.close()\n\n# Create animated GIF\nwith imageio.get_writer(\"kmeans_animation.gif\", mode=\"I\", duration=0.8) as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\nprint(\"GIF saved as kmeans_animation.gif\")\n\nGIF saved as kmeans_animation.gif"
  },
  {
    "objectID": "blog/project4/hw4_questions (1).html#a.-k-nearest-neighbors",
    "href": "blog/project4/hw4_questions (1).html#a.-k-nearest-neighbors",
    "title": "K-Means Clustering",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\nTo explore how the K-Nearest Neighbors (KNN) algorithm performs, we begin by generating a synthetic dataset with a non-linear classification boundary. The dataset includes two continuous features, x1 and x2, and a binary target variable y, where y is determined by whether a point lies above or below a wiggly boundary defined by a sine function: y = 1 if x2 &gt; sin(4 * x1) + x1, otherwise 0. This setup results in a flexible, non-linear separation that is well-suited for demonstrating the strengths and limitations of KNN in capturing local decision patterns without requiring a parametric model.\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed\nnp.random.seed(42)\n\n# Sample size\nn = 100\n\n# Generate features\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n\n# Define the wiggly boundary\nboundary = np.sin(4 * x1) + x1\n\n# Binary classification based on boundary\ny = (x2 &gt; boundary).astype(int)  # 1 if above the boundary, else 0\n\nTo visualize the synthetic classification task, we plot the generated data points along with the underlying wiggly decision boundary used to assign class labels. The horizontal axis represents x1, the vertical axis represents x2, and the class label y is indicated by color. Points above the boundary belong to class 1, while those below are labeled as class 0. This setup creates a challenging non-linear classification scenario that is well-suited to evaluating how K-Nearest Neighbors adapts to complex local structures.\n\n\n\n\n\nSynthetic Dataset with Wiggly Decision Boundary\n\n\n\n\nAs shown in the plot, the decision boundary is defined by the function x2 = sin(4 * x1) + x1, which creates a highly non-linear separation between the two classes. The points colored red (y=1) lie above this boundary, and the points colored blue (y=0) lie below it. This visualization confirms that the synthetic dataset contains significant curvature and overlap, which will test KNN’s ability to adapt its decision boundaries based on local neighborhoods rather than relying on a global linear rule.\n\nGenerating a Separate Test Dataset\nTo evaluate the performance of our K-Nearest Neighbors classifier, we generate a separate test dataset. This test set follows the same structure and decision boundary as the training data but is created using a different random seed to ensure that the points are independently sampled. By assessing model accuracy on this new data, we can determine how well KNN generalizes to unseen examples in a similarly distributed feature space.\n\nimport numpy as np\nimport pandas as pd\n\n# Set a different seed for independent test data\nnp.random.seed(24)\n\n# Sample size\nn_test = 100\n\n# Generate features\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\n\n# Define the wiggly boundary\nboundary_test = np.sin(4 * x1_test) + x1_test\n\n# Assign binary labels\ny_test = (x2_test &gt; boundary_test).astype(int)\n\n# Create test DataFrame\ndf_test = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\n\n# Preview first few rows\nprint(\"Preview of test dataset:\")\nprint(df_test.head())\n\nPreview of test dataset:\n         x1        x2  y\n0  2.760104  1.614976  0\n1  1.197072 -0.152494  0\n2  2.999204  0.114679  0\n3 -1.679596  2.665158  1\n4 -0.833662  0.861152  1\n\n\n\n\nImplementing K-Nearest Neighbors by Hand\nTo better understand how the K-Nearest Neighbors (KNN) algorithm works, we implement it from scratch using basic Python operations. This manual implementation allows us to see the underlying mechanics of distance-based classification and majority voting. After completing the custom implementation, we validate our results by comparing them to the output of scikit-learn’s KNeighborsClassifier, a widely used and optimized version of the algorithm. ### Implementing K-Nearest Neighbors by Hand\nTo deepen our understanding of how the K-Nearest Neighbors algorithm works, we implement it manually using only basic Python operations. This version computes Euclidean distances, finds the nearest neighbors, and performs majority voting for prediction. We then apply this implementation to the synthetic test dataset and print the predicted class labels.\n\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(p1, p2):\n    return np.sqrt(np.sum((p1 - p2)**2))\n\ndef knn_predict(X_train, y_train, X_test, k=5):\n    predictions = []\n    for test_point in X_test:\n        distances = [euclidean_distance(test_point, x) for x in X_train]\n        k_indices = np.argsort(distances)[:k]\n        k_labels = [y_train[i] for i in k_indices]\n        majority = Counter(k_labels).most_common(1)[0][0]\n        predictions.append(majority)\n    return np.array(predictions)\n\n\n\nPredict with Manual KNN and Print Results\n\n# Prepare training and test matrices\nX_train = np.column_stack((x1, x2))\ny_train = y\n\nX_test_vals = np.column_stack((x1_test, x2_test))\ny_test_vals = y_test\n\n# Run manual KNN prediction\ny_pred_manual = knn_predict(X_train, y_train, X_test_vals, k=5)\n\n# Print results\nprint(\"Manual KNN predictions (first 10):\")\nprint(y_pred_manual[:10])\n\nManual KNN predictions (first 10):\n[0 0 0 1 1 0 0 0 1 0]\n\n\nThe output above shows the first 10 predictions made by our manual K-Nearest Neighbors implementation. Each value corresponds to the predicted class label (0 or 1) for a test data point, based on the majority class among its 5 nearest neighbors in the training set. This confirms that the custom function is generating class predictions as expected, which can now be compared to the ground truth labels or validated against a standard library implementation.\n\n\nEvaluating Accuracy Across K Values\nTo determine the optimal number of neighbors for classification, we evaluate the accuracy of our manual KNN implementation across values of k from 1 to 30. For each k, we compute the percentage of correctly classified points in the test set and visualize the results. The goal is to identify which value of k yields the highest classification accuracy.\n\nimport matplotlib.pyplot as plt\n\nk_values = range(1, 31)\naccuracies = []\n\nfor k in k_values:\n    y_pred_k = knn_predict(X_train, y_train, X_test_vals, k=k)\n    accuracy = np.mean(y_pred_k == y_test_vals)\n    accuracies.append(accuracy * 100)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, accuracies, marker='o')\nplt.title(\"KNN Accuracy vs. K Value\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy (%)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe plot displays classification accuracy as a function of the number of neighbors used in KNN. The optimal `k` is the value that achieves the highest accuracy on the test dataset, providing the best generalization performance. This helps balance model complexity and prediction stability.\n\nBased on the accuracy plot, the optimal value of `k` is either **1 or 2**, as both yield the highest classification accuracy (~94%) on the test set. This suggests that very local decision boundaries perform best for this particular dataset."
  }
]