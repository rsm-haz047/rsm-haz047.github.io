[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nHanhua Zhu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nHanhua Zhu\n\n\nApr 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html",
    "href": "blog/project1/hw1_questions (4).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.\nThe experiment tested whether the presence of a matching donation offer (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the likelihood of giving and the amount donated. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the maximum match amount (e.g., $25,000, $50,000, or $100,000) and the suggested donation amount, which was calculated based on each recipient’s highest prior donation.\nRecipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.\nThe results were published in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate some of their core findings using the original dataset and statistical methods."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#introduction",
    "href": "blog/project1/hw1_questions (4).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.\nThe experiment tested whether the presence of a matching donation offer (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the likelihood of giving and the amount donated. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the maximum match amount (e.g., $25,000, $50,000, or $100,000) and the suggested donation amount, which was calculated based on each recipient’s highest prior donation.\nRecipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.\nThe results were published in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate some of their core findings using the original dataset and statistical methods."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#data",
    "href": "blog/project1/hw1_questions (4).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe load and explore the dataset from Karlan and List (2007)."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#description-1",
    "href": "blog/project1/hw1_questions (4).html#description-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description",
    "text": "Description\nThe dataset includes 50,083 observations, each representing a past donor who received a fundraising letter as part of a large-scale randomized field experiment. The dataset captures a wide range of information, including treatment assignments, donation outcomes, prior giving behavior, and demographic context.\nThere are 51 variables in total, which can be broadly grouped into the following categories: - Treatment indicators: such as treatment, ratio2, ratio3, size25, size50, size100, and askd1, which reflect match ratios, thresholds, and suggested donation amounts. - Outcome variables: including gave (binary indicator for donation) and amount (dollar amount donated). - Historical giving behavior: such as hpa (highest previous amount), ltmedmra (indicator for low prior donors), and years since the first donation. - Demographic and contextual features: such as female, couple, median_hhincome, page18_39, pwhite, and pop_propurban.\nMost variables are either complete or have minimal missingness, and the data types are a mix of binary indicators, integers, floats, and categorical values. This structure makes the dataset well-suited for regression, simulation, and visualization tasks.\nSummary statistics show that approximately 66.7% of observations are in the treatment group, with 33.3% in the control group, as expected from random assignment. The gave variable indicates that about 2.06% of individuals donated, and the average donation (amount) across the full sample is $0.92, with some contributions reaching as high as $400. This reflects the typical skew in donation behavior: most individuals gave nothing, while a few gave large amounts.\nThe match ratio flags (ratio2, ratio3) suggest roughly equal allocation across the 1:1, 2:1, and 3:1 groups. Likewise, the match threshold indicators (size25, size50, size100, sizeno) are each present in about 16.7% of the sample. Suggested donation framing variables (askd1, askd2, askd3) also appear to be evenly distributed, reinforcing that the experimental design was properly randomized.\nThe table below provides descriptions for key variables in the dataset.\n#:::: {.callout-note collapse=“true”}\n\n\n\n\n\n\n\n##\nVariable\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.666813\n0.471357\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncontrol\n50083.0\n0.333187\n0.471357\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nratio2\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nratio3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize25\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize50\n50083.0\n0.166623\n0.372643\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize100\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsizeno\n50083.0\n0.166743\n0.372750\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd1\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd2\n50083.0\n0.222291\n0.415790\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nask1\n50083.0\n71.501807\n101.728936\n25.000000\n35.000000\n45.000000\n65.000000\n1500.000000\n\n\nask2\n50083.0\n91.792724\n127.252628\n35.000000\n45.000000\n60.000000\n85.000000\n1875.000000\n\n\nask3\n50083.0\n111.046263\n151.673562\n50.000000\n55.000000\n70.000000\n100.000000\n2250.000000\n\n\namount\n50083.0\n0.915694\n8.707393\n0.000000\n0.000000\n0.000000\n0.000000\n400.000000\n\n\ngave\n50083.0\n0.020646\n0.142197\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\namountchange\n50083.0\n-52.672016\n1267.097778\n-200412.125000\n-50.000000\n-30.000000\n-25.000000\n275.000000\n\n\nhpa\n50083.0\n59.384975\n71.179871\n0.000000\n30.000000\n45.000000\n60.000000\n1000.000000\n\n\nltmedmra\n50083.0\n0.493720\n0.499966\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nfreq\n50083.0\n8.039355\n11.394454\n0.000000\n2.000000\n4.000000\n10.000000\n218.000000\n\n\nyears\n50082.0\n6.097540\n5.503492\n0.000000\n2.000000\n5.000000\n9.000000\n95.000000\n\n\nyear5\n50083.0\n0.508815\n0.499927\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nmrm2\n50082.0\n13.007268\n12.081403\n0.000000\n4.000000\n8.000000\n19.000000\n168.000000\n\n\ndormant\n50083.0\n0.523471\n0.499454\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfemale\n48972.0\n0.277669\n0.447854\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\ncouple\n48935.0\n0.091897\n0.288884\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nstate50one\n50083.0\n0.000998\n0.031581\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nnonlit\n49631.0\n2.473918\n1.961528\n0.000000\n1.000000\n3.000000\n4.000000\n6.000000\n\n\ncases\n49631.0\n1.499768\n1.155140\n0.000000\n1.000000\n1.000000\n2.000000\n4.000000\n\n\nstatecnt\n50083.0\n5.998820\n5.745993\n0.001995\n1.833234\n3.538799\n9.607021\n17.368841\n\n\nstateresponse\n50083.0\n0.020627\n0.005171\n0.000000\n0.018163\n0.019710\n0.023048\n0.076923\n\n\nstateresponset\n50083.0\n0.021989\n0.006257\n0.000000\n0.018493\n0.021697\n0.024703\n0.111111\n\n\nstateresponsec\n50080.0\n0.017717\n0.007516\n0.000000\n0.012862\n0.019881\n0.020806\n0.052632\n\n\nstateresponsetminc\n50080.0\n0.004273\n0.009112\n-0.047619\n-0.001388\n0.001779\n0.010545\n0.111111\n\n\nperbush\n50048.0\n0.487940\n0.078733\n0.090909\n0.444444\n0.484848\n0.525253\n0.731959\n\n\nclose25\n50048.0\n0.185702\n0.388870\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nred0\n50048.0\n0.404452\n0.490791\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nblue0\n50048.0\n0.595548\n0.490791\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nredcty\n49978.0\n0.510245\n0.499900\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nbluecty\n49978.0\n0.488715\n0.499878\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\npwhite\n48217.0\n0.819599\n0.168560\n0.009418\n0.755845\n0.872797\n0.938827\n1.000000\n\n\npblack\n48047.0\n0.086710\n0.135868\n0.000000\n0.014729\n0.036554\n0.090882\n0.989622\n\n\npage18_39\n48217.0\n0.321694\n0.103039\n0.000000\n0.258311\n0.305534\n0.369132\n0.997544\n\n\nave_hh_sz\n48221.0\n2.429012\n0.378105\n0.000000\n2.210000\n2.440000\n2.660000\n5.270000\n\n\nmedian_hhincome\n48209.0\n54815.700533\n22027.316665\n5000.000000\n39181.000000\n50673.000000\n66005.000000\n200001.000000\n\n\npowner\n48214.0\n0.669418\n0.193405\n0.000000\n0.560222\n0.712296\n0.816798\n1.000000\n\n\npsch_atlstba\n48215.0\n0.391661\n0.186599\n0.000000\n0.235647\n0.373744\n0.530036\n1.000000\n\n\npop_propurban\n48217.0\n0.871968\n0.258633\n0.000000\n0.884929\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n::::\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo assess whether the treatment was randomly assigned, I examine five pre-treatment covariates unrelated to the outcome: years (years since first donation), female (gender indicator), couple (marital status), median_hhincome (median household income by zip code), and page18_39 (proportion aged 18–39 in the zip code). These are useful for balance testing because they should not be affected by treatment assignment and reflect characteristics likely observed before the intervention.\nFor each variable, I conduct:\n\nA manual t-test, calculated using the following formula:\n\n\\[\nt = \\frac{\\bar{X}_{\\text{treatment}} - \\bar{X}_{\\text{control}}}{\\sqrt{\\frac{s^2_{\\text{treatment}}}{n_{\\text{treatment}}} + \\frac{s^2_{\\text{control}}}{n_{\\text{control}}}}}\n\\]\n\nA linear regression, where the covariate is regressed on the treatment indicator:\n\n\\[\n\\text{covariate}_i = \\alpha + \\beta \\cdot \\text{treatment}_i + \\varepsilon_i\n\\]\nThe coefficient \\(\\beta\\) captures the estimated difference in means between groups.\n\n\n\nManual T-Test Results\n\nyears: The t-statistic was -1.091, indicating no significant difference.\nfemale: The t-statistic was -1.754, just below conventional significance.\ncouple: The t-statistic was -0.582, suggesting no group difference in relationship status.\nmedian_hhincome: The t-statistic was -0.743, showing no income-based imbalance.\npage18_39: The t-statistic was -0.124, indicating age composition is well balanced.\n\n\n\n\nLinear Regression Results\nEach covariate was also regressed on the treatment variable:\n\nyears: Coefficient = -0.058, p = 0.270\nfemale: Coefficient = -0.008, p = 0.079\ncouple: Coefficient = -0.002, p = 0.559\nmedian_hhincome: Coefficient = -157.925, p = 0.458\npage18_39: Coefficient = ~0.000, p = 0.901\n\n\n\n\nInterpretation\nAcross all covariates, both the t-tests and linear regressions provide consistent evidence of no statistically significant differences between the treatment and control groups. The estimated coefficients are small, and all p-values exceed 0.05, supporting the assumption of balance under random assignment.\nThese results replicate the purpose of Table 1 in Karlan and List (2007), which provides confidence that the estimated treatment effects are not confounded by observable pre-treatment characteristics."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#experimental-results",
    "href": "blog/project1/hw1_questions (4).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCharitable Contribution Made\nWe examine whether the treatment group had a higher donation rate compared to the control group.\n\n\n\n\n\n\n\n\n\nThe bar chart above compares the average donation rates between individuals in the control group and those in the treatment group. The control group received a standard fundraising letter, while the treatment group received a letter that included a matching donation offer.\nAlthough the overall donation rates are low in both groups, the treatment group shows a visibly higher proportion of individuals who donated. This difference, though modest, is consistent with the hypothesis that matching gifts encourage charitable giving. The visual evidence aligns with the statistical results from t-tests and regressions presented later in the analysis, and replicates the main findings of Karlan and List (2007), where even small psychological nudges—like a matching offer—can lead to meaningful behavioral changes.\n\nT-Test for Donation Rates\nThis code performs an independent samples t-test to determine whether the mean donation rate (gave) differs significantly between the treatment and control groups. Since gave is a binary outcome indicating whether a donation was made, the test compares the proportion of donors across the two groups to assess the effect of receiving a matching donation offer.\n\n# T-test on binary outcome (gave)\ntreat = data[data['treatment'] == 1]['gave']\ncontrol = data[data['treatment'] == 0]['gave']\n\nt_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)\nprint(f\"T-test for 'gave':\\nT-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\n\nT-test for 'gave':\nT-statistic: 3.209, p-value: 0.001\n\n\nThe result of the t-test yields a t-statistic of 3.209 with a p-value of 0.001. This indicates a statistically significant difference in donation rates between the treatment and control groups at the 1% significance level. In other words, individuals who received a matching donation offer were significantly more likely to donate than those who did not, supporting the hypothesis that such offers increase giving behavior.\nTo complement the t-test, we estimate a simple ordinary least squares (OLS) regression to evaluate the effect of the treatment assignment on the probability of giving. The binary outcome variable gave is regressed on the treatment indicator, which equals 1 for those who received a matching donation offer and 0 for the control group.\nThis model can be interpreted as a comparison of group means, where the intercept represents the donation rate in the control group, and the treatment coefficient captures the average difference in donation rates between groups.\n\n# Bivariate OLS regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=data).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n9.618\n\n\nDate:\nWed, 07 May 2025\nProb (F-statistic):\n0.00193\n\n\nTime:\n21:37:35\nLog-Likelihood:\n26630.\n\n\nNo. Observations:\n50083\nAIC:\n-5.326e+04\n\n\nDf Residuals:\n50081\nBIC:\n-5.324e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\ntreatment\n0.0042\n0.001\n3.101\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\n\nOmnibus:\n59814.280\nDurbin-Watson:\n2.005\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4317152.727\n\n\nSkew:\n6.740\nProb(JB):\n0.00\n\n\nKurtosis:\n46.440\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe results confirm what we observed in the t-test. The coefficient on the treatment variable is 0.0042, with a p-value of 0.002, indicating that the difference in donation rates between treatment and control is statistically significant at the 1% level.\nThis suggests that receiving a matching donation offer increased the likelihood of giving by approximately 0.42 percentage points. While this effect may seem small in absolute terms, it is consistent with the behavioral insights from Karlan and List (2007): even modest framing changes like mentioning a match can lead to meaningful increases in donor participation.\n\n# Probit regression: gave ~ treatment\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\nprobit_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 07 May 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:37:35\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\n\nInterpretation of Experimental Results\nThe evidence suggests that being assigned to the treatment group — receiving a matching donation offer — significantly increased the likelihood of making a charitable donation.\n\nThe bar plot shows a noticeable difference in donation rates between the treatment and control groups.\nThe t-test reveals a statistically significant difference in donation behavior (T = 3.209, p = 0.001), meaning the treatment group donated at a higher rate than the control group.\nThe OLS regression confirms this finding. The treatment coefficient is 0.0042, with a p-value of 0.002, indicating that assignment to treatment increases the probability of donating by about 0.42 percentage points.\nThe Probit regression also supports this: the coefficient on treatment is 0.0868 (p = 0.002), again showing a positive and statistically significant effect on the likelihood of giving.\n\nOverall, these results replicate the findings in Karlan and List (2007): matching donations increase the probability of giving, even if the actual increase is modest in absolute terms. The consistency across all three statistical approaches (t-test, OLS, and probit) strengthens the causal interpretation of the results. This supports the idea that even small nudges like matching offers can influence donor behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nDifferences between Match Rates\nWe explore whether larger match ratios (e.g., 2:1 or 3:1) result in higher likelihoods of donating compared to a 1:1 match.\n\n\n\nT-tests for Match Ratios\n\n# Filter only treatment group and each match ratio\nratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']\nratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']\nratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']\n\n# T-tests\nprint(\"2:1 vs 1:1\")\nprint(stats.ttest_ind(ratio2, ratio1, equal_var=False))\n\nprint(\"\\n3:1 vs 2:1\")\nprint(stats.ttest_ind(ratio3, ratio2, equal_var=False))\n\n2:1 vs 1:1\nTtestResult(statistic=0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n\n3:1 vs 2:1\nTtestResult(statistic=0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\n\nInterpretation:\nThese t-tests compare donation rates for each match ratio within the treatment group.\n- The 2:1 vs 1:1 test shows a [insert p-value here], and\n- The 3:1 vs 2:1 test shows a [insert p-value here].\nIn both comparisons, the p-values are relatively large, suggesting no statistically significant difference in donation likelihood across match sizes. This supports Karlan and List’s finding that increasing the match ratio does not meaningfully improve the effectiveness of the match offer.\n\n\n\n\nRegression on Match Ratio (Categorical)\n\n# Filter for treatment group and valid match ratios\nmatch_data = data[(data['treatment'] == 1) & (data['ratio'].isin([1, 2, 3]))].copy()\n\n# Properly set 'ratio' as a categorical variable with 1:1 as the baseline\nmatch_data['ratio'] = pd.Categorical(match_data['ratio'], categories=[1, 2, 3])\n\n# Run the regression: donation behavior by match ratio\nmodel = smf.ols(\"gave ~ C(ratio)\", data=match_data).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 07 May 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:37:35\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nC(ratio)[T.2]\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nC(ratio)[T.3]\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nInterpretation:\nThe regression estimates donation rates for each match ratio using the 1:1 match as the baseline (intercept = 2.07%).\n- The coefficient for 2:1 is 0.0019 (p = 0.338), and\n- The coefficient for 3:1 is 0.0020 (p = 0.313).\nThese coefficients are very small and not statistically significant, indicating that higher match ratios do not meaningfully increase the likelihood of giving beyond what is achieved with a 1:1 match.\nThis supports the key finding in Karlan and List (2007): the existence of a match offer increases donations, but increasing the ratio (e.g., from 1:1 to 3:1) does not further boost effectiveness.\n\n\n\nResponse Rate Differences from Raw Data and Model\n\n# Group-level mean donation rates\nratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']\nratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']\nratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']\n\nmean1 = ratio1.mean()\nmean2 = ratio2.mean()\nmean3 = ratio3.mean()\n\nprint(f\"Response Rate 1:1 = {mean1:.4f}\")\nprint(f\"Response Rate 2:1 = {mean2:.4f}\")\nprint(f\"Response Rate 3:1 = {mean3:.4f}\")\n\nprint(f\"\\n2:1 - 1:1 = {mean2 - mean1:.4f}\")\nprint(f\"3:1 - 2:1 = {mean3 - mean2:.4f}\")\n\nResponse Rate 1:1 = 0.0207\nResponse Rate 2:1 = 0.0226\nResponse Rate 3:1 = 0.0227\n\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\n\n# Model-based differences from regression coefficients\ncoef = model.params\n\nprint(\"\\nModel-Based Differences:\")\nprint(f\"2:1 - 1:1 = {coef['C(ratio)[T.2]']:.4f}\")\nprint(f\"3:1 - 1:1 = {coef['C(ratio)[T.3]']:.4f}\")\nprint(f\"3:1 - 2:1 = {coef['C(ratio)[T.3]'] - coef['C(ratio)[T.2]']:.4f}\")\n\n\nModel-Based Differences:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\n\n\nInterpretation:\nBoth the raw response rate differences and the model-based regression estimates suggest that increasing the match ratio from 1:1 to 2:1 leads to a very small increase in the probability of giving (approximately 0.0019, or 0.19 percentage points).\nIncreasing the ratio further from 2:1 to 3:1 adds virtually no additional effect — a difference of just 0.0001, or 0.01 percentage points. These differences are not only small in magnitude but also statistically insignificant based on earlier t-tests and regression outputs.\nThe fact that the raw differences and regression coefficients are nearly identical provides strong evidence that these small effects are consistent across estimation approaches.\nIn line with Karlan and List (2007), we conclude that the presence of a match offer matters, but larger match ratios (like 3:1) do not provide additional benefit over a standard 1:1 match. This supports their interpretation that psychological or motivational factors may be triggered by any match, not necessarily a more generous one.\n\n\n\n\n\nSummary\n\n\n\n\n\n\nConclusion\n\n\n\nNeither the t-tests nor the regression provide strong evidence that increasing the match ratio from 1:1 to 2:1 or 3:1 significantly increases the donation response rate. The estimated differences are small and statistically insignificant. These findings replicate Karlan and List’s claim on page 8 that “larger match ratios do not provide additional lift,” and suggest that announcing any match offer may be sufficient to nudge donation behavior — but increasing the ratio adds little extra power."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#size-of-charitable-contribution",
    "href": "blog/project1/hw1_questions (4).html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, I analyze the effect of the matching donation offer on the amount donated, not just whether individuals gave. I examine this in two ways: (1) using the full sample including non-donors, and (2) restricting the analysis to only those who made a donation.\n\n\nFull Sample Regression\nThe first model regresses the donation amount (amount) on the treatment assignment across the entire sample, including non-donors (who are coded as 0 for amount). This allows us to assess the average treatment effect on giving across all individuals.\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.461\n\n\nDate:\nWed, 07 May 2025\nProb (F-statistic):\n0.0628\n\n\nTime:\n21:37:35\nLog-Likelihood:\n-1.7946e+05\n\n\nNo. Observations:\n50083\nAIC:\n3.589e+05\n\n\nDf Residuals:\n50081\nBIC:\n3.589e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.8133\n0.067\n12.063\n0.000\n0.681\n0.945\n\n\ntreatment\n0.1536\n0.083\n1.861\n0.063\n-0.008\n0.315\n\n\n\n\n\n\n\n\nOmnibus:\n96861.113\nDurbin-Watson:\n2.008\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n240735713.635\n\n\nSkew:\n15.297\nProb(JB):\n0.00\n\n\nKurtosis:\n341.269\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression results show that the intercept (average donation in the control group) is $0.8133, while the treatment effect is estimated at $0.1536. The p-value for the treatment coefficient is 0.063, which is marginally significant at the 10% level.\nThis suggests that receiving a matching offer may lead to a small increase in the average donation amount across the full sample, but the evidence is not strong enough to be statistically significant at conventional 5% thresholds.\n\n\n\nRegression Among Donors Only\nNext, I restrict the sample to individuals who actually made a donation (gave == 1) to test whether the matching offer affected how much donors gave, conditional on donating.\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nWed, 07 May 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n21:37:35\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAmong donors, the average donation in the control group is approximately $45.54, as reflected in the intercept. The treatment group gave on average $1.67 less than the control group, though this difference is not statistically significant (p = 0.561).\n\n\n\nInterpretation\nThese results indicate that while the treatment may increase the likelihood of giving (as seen earlier), it does not significantly affect the amount donated. In the full sample, the treatment effect is small and only marginally significant. Among those who donated, the estimated effect is negative and clearly not statistically significant.\nBecause the treatment was randomly assigned, the coefficient estimates can be interpreted causally. However, it is important to note that the donor-only model is conditional on post-treatment behavior and may be affected by selection.\nOverall, this analysis supports the main takeaway from Karlan and List (2007): matching offers increase participation (extensive margin), but not the size of contributions (intensive margin).\n\nHistogram of Donation Amounts: Treatment Group\n\n\n\n\n\n\n\n\n\n\n\nHistogram of Donation Amounts: Control Group\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nIn the regression using the full sample, we find that the treatment group donated $0.15 more on average than the control group. However, this result is not statistically significant (p = 0.063), meaning we cannot rule out that the difference is due to random chance.\nIn the donor-only regression, we isolate just the people who made a donation and ask: do those in the treatment group give more than those in the control? Here, the treatment coefficient is -1.67, again not statistically significant (p = 0.561). In fact, the sign of the effect is slightly negative.\nThe histograms show that donation amounts are heavily skewed, with most donations under $100. The means for treatment and control are both around $45–46, with very similar distributions.\nConclusion: The treatment (i.e., receiving a matching offer) appears to influence whether people give, but not how much they give. The full-sample regression can be interpreted causally because of random assignment, while the donor-only regression cannot, since it conditions on a post-treatment behavior (selection bias).\nOverall, our findings replicate Karlan and List (2007): matching gifts increase response rates, but do not significantly affect the donation size conditional on giving."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#simulation-experiment",
    "href": "blog/project1/hw1_questions (4).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nWe simulate two large samples: - One from the control group (donation probability = 0.018) - One from the treatment group (donation probability = 0.022)\nWe calculate the difference between matched pairs, and then compute the cumulative average difference.\n\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability ( p = 0.018 )), and one representing those who did (treatment, with ( p = 0.022 )). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the cumulative average of these differences.\n\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability ( p = 0.018 )), and one representing those who did (treatment, with ( p = 0.022 )). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the cumulative average of these differences.\n\n\nLaw of Large Numbers\nTo simulate the Law of Large Numbers, I compare paired random samples from two Bernoulli distributions: one for the control group (( p = 0.018 )) and one for the treatment group (( p = 0.022 )). I compute the difference in outcomes for each pair and plot the cumulative average to see whether it converges to the true mean difference of 0.004.\n\n\nLaw of Large Numbers: Convergence of Mean Difference\n\n\n\n\n\n\n\n\n\nAs shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of 0.004. This pattern exemplifies the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nThis visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.\nAs shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of 0.004. This pattern exemplifies the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nThis visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.\n\n\n\nInterpretation\n\nThe simulation demonstrates the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nIn this plot, the cumulative average difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups begins with wide fluctuations due to randomness in the early observations. However, as we simulate more and more matched pairs (up to 100,000), the cumulative difference stabilizes and converges to the true expected difference of 0.004.\nThis reinforces a key idea in statistics: with large enough sample sizes, the law of averages ensures reliable estimates of population parameters. It also explains why Karlan and List were able to detect small treatment effects — because their experiment had tens of thousands of observations. ```\n\n\n\nCentral Limit Theorem\nWe simulate distributions of average differences in donation rates between control (p = 0.018) and treatment (p = 0.022) groups, using increasing sample sizes (n = 50, 200, 500, 1000). For each sample size, we repeat the process 1000 times and visualize the distribution of sample differences.\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nThese four histograms illustrate the Central Limit Theorem using simulated sample means of donation rate differences between the treatment (p = 0.022) and control (p = 0.018) groups.\nAs the sample size increases: - With n = 50, the distribution is wide and noisy — there’s considerable variation, and the true mean difference (0.004) is not clearly distinguishable from zero. - At n = 200, the distribution begins to resemble a bell curve, and the true difference is more centered. - At n = 500, the histogram tightens noticeably, and zero begins to lie closer to the tail of the distribution. - By n = 1000, the sample mean differences are tightly clustered around 0.004, and zero is clearly no longer at the center — it lies in the tail, suggesting strong evidence against the null of no effect.\nThis simulation confirms that larger samples reduce sampling variability and make it easier to detect small, true differences — exactly what Karlan and List leveraged with their large-scale experiment."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hanhua Zhu",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset containing information on engineering firms and their patenting activity. This includes whether or not a firm uses Blueprinty’s software.\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe first few rows of the dataset show each firm’s number of patents, regional location, age, and whether they are a Blueprinty customer. We see a mix of customer (iscustomer = 1) and non-customer firms, with patent counts ranging from 0 to 4. Firm ages vary from around 24 to 38 years, and regions span categories like Midwest and Southwest. These observations suggest the data is well-structured for count-based modeling and that customer status, region, and firm age may all influence patenting outcomes.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe? ## Patent Counts by Customer Status\nTo understand whether firms using Blueprinty’s software tend to receive more patents, we examine the distribution of patent counts by customer status. The plot below shows histograms for customers and non-customers side by side, and the table reports the average number of patents for each group.\n\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of patent counts for firms that do and do not use Blueprinty’s software. While the majority of both groups have relatively few patents, we observe that Blueprinty customers are more likely to appear in higher patent count categories (e.g., 5 or more patents). In contrast, non-customers dominate the lower count bins (0–3 patents). This suggests that firms using Blueprinty’s software may be associated with higher patenting activity, though further analysis is needed to account for other factors like firm age and region.\n\n\n\n\n\n\n\n\n\niscustomer\nAverage Number of Patents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 0.66 more patents than non-customers. While this difference is modest, it suggests a positive association between using the software and patenting activity. This pattern aligns with what we observed in the histogram and sets the stage for more rigorous modeling using Poisson regression.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset containing information on engineering firms and their patenting activity. This includes whether or not a firm uses Blueprinty’s software.\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe first few rows of the dataset show each firm’s number of patents, regional location, age, and whether they are a Blueprinty customer. We see a mix of customer (iscustomer = 1) and non-customer firms, with patent counts ranging from 0 to 4. Firm ages vary from around 24 to 38 years, and regions span categories like Midwest and Southwest. These observations suggest the data is well-structured for count-based modeling and that customer status, region, and firm age may all influence patenting outcomes.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe? ## Patent Counts by Customer Status\nTo understand whether firms using Blueprinty’s software tend to receive more patents, we examine the distribution of patent counts by customer status. The plot below shows histograms for customers and non-customers side by side, and the table reports the average number of patents for each group.\n\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of patent counts for firms that do and do not use Blueprinty’s software. While the majority of both groups have relatively few patents, we observe that Blueprinty customers are more likely to appear in higher patent count categories (e.g., 5 or more patents). In contrast, non-customers dominate the lower count bins (0–3 patents). This suggests that firms using Blueprinty’s software may be associated with higher patenting activity, though further analysis is needed to account for other factors like firm age and region.\n\n\n\n\n\n\n\n\n\niscustomer\nAverage Number of Patents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 0.66 more patents than non-customers. While this difference is modest, it suggests a positive association between using the software and patenting activity. This pattern aligns with what we observed in the histogram and sets the stage for more rigorous modeling using Poisson regression.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#differences-in-firm-characteristics-by-customer-status",
    "href": "blog/project2/hw2_questions.html#differences-in-firm-characteristics-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Differences in Firm Characteristics by Customer Status",
    "text": "Differences in Firm Characteristics by Customer Status\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in characteristics like region and firm age between customers and non-customers.\nWe begin by examining whether the age distribution differs by customer status, and then compare regional distributions.\n\n\n/var/folders/n6/y_4fsn553lq6_fzmfb11cjth0000gn/T/ipykernel_78323/1514657809.py:6: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot reveals that Blueprinty customers tend to be slightly younger than non-customer firms. While both groups share similar age ranges and interquartile spreads, the median firm age is lower among customers. This suggests that firm age may influence both software adoption and patenting behavior. As a result, it is important to control for age in subsequent modeling to ensure we isolate the effect of using Blueprinty’s software from differences attributable to firm maturity.\nThe table shows the proportion of Blueprinty customers and non-customers across different regions. Most regions — including the Midwest, Northwest, South, and Southwest — have a relatively low customer adoption rate (about 16–18%). However, the Northeast stands out: a majority of firms in this region (55%) are Blueprinty customers.\n\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.83\n0.17\n\n\nNortheast\n0.45\n0.55\n\n\nNorthwest\n0.84\n0.16\n\n\nSouth\n0.82\n0.18\n\n\nSouthwest\n0.82\n0.18\n\n\n\n\n\n\n\nThis regional disparity suggests that firms in the Northeast are significantly more likely to use Blueprinty’s software. As a result, region may confound the relationship between software usage and patent success, and it should be accounted for in any regression model to avoid biased estimates.\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\). ### Poisson Likelihood Function\nLet ( Y_i (_i) ), where ( _i ) is the expected number of patents for firm ( i ).\nThe probability mass function for a Poisson-distributed outcome is:\n[ f(Y_i | _i) = ]\nAssuming observations are independent across ( n ) firms, the likelihood function is:\n[ () = _{i=1}^n ]\nTaking the log gives the log-likelihood:\n[ () = _{i=1}^n ( -_i + Y_i _i - Y_i! ) ]\nIn Poisson regression, we model ( _i = (X_i ) ), which ensures ( _i &gt; 0 ).\n\n\nCoding the Log-Likelihood Function\nWe define the log-likelihood function for a Poisson model, where the outcome variable ( Y ) follows a Poisson distribution with mean ( ). This function will be useful for implementing MLE manually.\n\nimport numpy as np\n\ndef poisson_loglikelihood(lam, y):\n    \"\"\"\n    Compute the log-likelihood for Poisson-distributed outcomes.\n    \n    Parameters:\n    - lam: array-like of expected values (lambda_i)\n    - y: array-like of observed counts (Y_i)\n    \n    Returns:\n    - total log-likelihood (float)\n    \"\"\"\n    lam = np.asarray(lam)\n    y = np.asarray(y)\n    \n    loglik = np.sum(-lam + y * np.log(lam) - np.log(np.math.factorial(y)))\n    return loglik\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n\nLog-Likelihood over a Range of λ Values\nWe use our Poisson log-likelihood function to evaluate how the likelihood changes over different values of λ. This helps us visualize the value of λ that best fits the observed number of patents.\n\n\n\n\n\n\n\n\n\nThe plot above shows how the Poisson log-likelihood varies with different values of λ, which represents the expected number of patents per firm. The curve peaks around λ = 4, indicating that this value maximizes the likelihood of observing the data under a Poisson model. In other words, the value λ ≈ 4 is the maximum likelihood estimate (MLE) — it best fits the distribution of observed patent counts. Values of λ much smaller or larger result in significantly lower likelihoods, confirming that λ ≈ 4 provides the most plausible explanation for the data.\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda. ### Deriving the MLE for λ in the Poisson Model\nLet ( Y_1, Y_2, , Y_n ) be independent observations from a Poisson distribution with parameter ( ). The log-likelihood function is:\n[ () = _{i=1}^n ( -+ Y_i - Y_i! ) ]\nTo find the maximum likelihood estimate (MLE), take the derivative with respect to ( ), set it to zero, and solve:\n[ = _{i=1}^n ( -1 + ) = 0 ]\n[ -n + _{i=1}^n Y_i = 0 ]\n[ {i=1}^n Y_i = n = {i=1}^n Y_i = {Y} ]\nSo, the MLE of ( ) is simply the sample mean of the observed data:\n[ _{} = {Y} ]\nThis result makes intuitive sense because the Poisson distribution has a single parameter, ( ), which represents both its mean and variance.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\nFinding the MLE of λ Using Optimization\nTo confirm our visual result and mathematical derivation, we numerically estimate the MLE of ( ) by maximizing the Poisson log-likelihood. Since most optimizers minimize functions, we minimize the negative log-likelihood using scipy.optimize.\n\nfrom scipy.optimize import minimize_scalar\n\n# Define the negative log-likelihood function\ndef neg_loglik(lambda_val):\n    return -poisson_loglikelihood(lambda_val, y_observed)\n\n# Optimize using bounded scalar minimization\nresult = minimize_scalar(neg_loglik, bounds=(0.1, 10), method='bounded')\n\n# Extract the MLE\nlambda_mle = result.x\nlambda_mle\n\n3.6846662261327716\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nDefining the Poisson Regression Log-Likelihood\nIn the Poisson regression model, each firm ( i ) has a rate ( _i = (X_i^) ), where ( X_i ) includes the firm’s age, age squared, region dummies, and customer status. We now define the log-likelihood function in terms of ( ) and ( X ).\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, y, X):\n    \"\"\"\n    Compute the Poisson regression log-likelihood.\n\n    Parameters:\n    - beta: array-like, shape (k,)\n    - y: array-like, shape (n,)\n    - X: array-like, shape (n, k)\n\n    Returns:\n    - log-likelihood (float)\n    \"\"\"\n    beta = np.asarray(beta)\n    y = np.asarray(y)\n    X = np.asarray(X)\n\n    lambda_i = np.exp(X @ beta)  # inverse link function\n    loglik = np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n    return loglik\n\n\n\nEstimating a Poisson Regression Model with statsmodels\nWe now estimate the Poisson regression using statsmodels, which provides built-in handling of the link function, robust inference, and well-formatted output.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Create age^2 and region dummies\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Combine predictors\nX = pd.concat([\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX = sm.add_constant(X)  # add intercept\n\n# Convert both X and y to float arrays (guaranteed numeric)\nX = X.astype(float)\ny = df[\"patents\"].astype(float)\n\n# Fit Poisson regression\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Show model summary\npoisson_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2143.3\n\n\nTime:\n21:37:40\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nNortheast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nNorthwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nSouth\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nSouthwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nVerifying Results Using statsmodels.GLM\nTo verify our custom likelihood and optimization results, we fit the same Poisson regression model using the built-in statsmodels.GLM() function with the Poisson family. This method handles link functions and inference automatically, providing coefficient estimates and standard errors.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Create age squared and dummy variables\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Build the design matrix\nX = pd.concat([\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX = sm.add_constant(X)  # Add intercept\n\n# Convert to float (ensures all numeric types)\nX = X.astype(float)\ny = df[\"patents\"].astype(float)\n\n# Fit Poisson model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display the summary\npoisson_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2143.3\n\n\nTime:\n21:37:40\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nNortheast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nNorthwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nSouth\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nSouthwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nhe regression results provide strong evidence that using Blueprinty’s software is associated with an increase in the expected number of patents. The coefficient on iscustomer is 0.2076 and is statistically significant (p &lt; 0.001), implying that being a customer increases the log expected patent count. Exponentiating this coefficient (i.e., 𝑒 0.2076 ≈ 1.23 e 0.2076 ≈1.23) suggests that, all else equal, Blueprinty customers have about 23% more patents than non-customers.\nFirm age is also significant: the positive coefficient on age and negative coefficient on age_squared indicate a concave (inverted-U) relationship between age and patent output — suggesting patent productivity increases with age, but at a decreasing rate.\nRegional variables are not statistically significant, indicating no clear evidence that location alone explains differences in patenting after controlling for other factors.\nOverall, the model supports Blueprinty’s claim that their customers are more successful in patenting, even when accounting for differences in firm age and location.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\nSimulated Impact of Blueprinty’s Software\nWe now simulate the impact of Blueprinty by predicting patent counts under two scenarios: (1) no firms use the software, and (2) all firms use the software. We then compute the average difference in predicted patents across all firms.\n\n\n0.792768071045253\n\n\nTo assess the practical impact of Blueprinty’s software on patenting outcomes, we simulated two scenarios using our fitted Poisson regression model. In the first scenario, we predicted the number of patents for each firm assuming none used Blueprinty. In the second, we predicted patent counts assuming all firms were customers. The average difference between these two sets of predictions was approximately 0.79 patents per firm, indicating that Blueprinty’s software is associated with a meaningful increase in patenting success. This approach translates the regression results into an interpretable quantity and confirms that, on average, firms that use Blueprinty are expected to secure nearly one additional patent over five years compared to similar non-customers."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#modeling-airbnb-reviews",
    "href": "blog/project2/hw2_questions.html#modeling-airbnb-reviews",
    "title": "Poisson Regression Examples",
    "section": "Modeling Airbnb Reviews",
    "text": "Modeling Airbnb Reviews\nWe assume that the number of reviews is a good proxy for the number of bookings a listing receives. In this section, we conduct exploratory data analysis, prepare the dataset, fit a Poisson regression model, and interpret the model results.\n\n1. Data Cleaning and Exploration\nWe begin by selecting relevant variables for predicting the number of reviews. We drop any rows with missing values to ensure clean model fitting.\n\nimport pandas as pd\n\n# Load the Airbnb dataset (adjust the path as needed)\ndf = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project2/airbnb.csv\")\n\n# Preview column names to confirm structure\ndf.columns\n\nIndex(['Unnamed: 0', 'id', 'days', 'last_scraped', 'host_since', 'room_type',\n       'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n       'review_scores_cleanliness', 'review_scores_location',\n       'review_scores_value', 'instant_bookable'],\n      dtype='object')\n\n\n\n# Select relevant variables\nairbnb = df[[\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \n    \"review_scores_value\", \"instant_bookable\"\n]]\n\n# Drop missing values\nairbnb = airbnb.dropna()\n\n# Display a summary of the cleaned data\nairbnb.describe(include=\"all\")\n\n\n\n\n\n\n\n\nnumber_of_reviews\nroom_type\nbathrooms\nbedrooms\nprice\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\ncount\n30160.000000\n30160\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160\n\n\nunique\nNaN\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\ntop\nNaN\nEntire home/apt\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nf\n\n\nfreq\nNaN\n15543\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n24243\n\n\nmean\n21.170889\nNaN\n1.122132\n1.151459\n140.206863\n9.201724\n9.415351\n9.333952\nNaN\n\n\nstd\n32.007541\nNaN\n0.384916\n0.699010\n188.392314\n1.114261\n0.843185\n0.900472\nNaN\n\n\nmin\n1.000000\nNaN\n0.000000\n0.000000\n10.000000\n2.000000\n2.000000\n2.000000\nNaN\n\n\n25%\n3.000000\nNaN\n1.000000\n1.000000\n70.000000\n9.000000\n9.000000\n9.000000\nNaN\n\n\n50%\n8.000000\nNaN\n1.000000\n1.000000\n103.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n75%\n26.000000\nNaN\n1.000000\n1.000000\n169.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\nmax\n421.000000\nNaN\n6.000000\n10.000000\n10000.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n\n\n\n\n\n\n\n2. Poisson Regression Model\nWe use Poisson regression to model the number of reviews as a function of listing features, assuming that reviews follow a Poisson distribution. We include dummy variables for categorical features and a constant term.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Create dummy variables\ndummies = pd.get_dummies(airbnb[[\"room_type\", \"instant_bookable\"]], drop_first=True)\n\n# Build design matrix\nX_airbnb = pd.concat([\n    airbnb[[\"bathrooms\", \"bedrooms\", \"price\", \n            \"review_scores_cleanliness\", \n            \"review_scores_location\", \n            \"review_scores_value\"]],\n    dummies\n], axis=1)\n\nX_airbnb = sm.add_constant(X_airbnb)\n\n# Ensure all data are numeric\nX_airbnb = X_airbnb.astype(float)\ny_airbnb = airbnb[\"number_of_reviews\"].astype(float)\n\n# Fit Poisson regression\nmodel_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson()).fit()\n\n# Display model summary\nmodel_airbnb.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30150\n\n\nModel Family:\nPoisson\nDf Model:\n9\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2900e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n9.3653e+05\n\n\nTime:\n21:37:40\nPearson chi2:\n1.41e+06\n\n\nNo. Iterations:\n6\nPseudo R-squ. (CS):\n0.5649\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.5725\n0.016\n223.215\n0.000\n3.541\n3.604\n\n\nbathrooms\n-0.1240\n0.004\n-33.091\n0.000\n-0.131\n-0.117\n\n\nbedrooms\n0.0749\n0.002\n37.698\n0.000\n0.071\n0.079\n\n\nprice\n-1.435e-05\n8.3e-06\n-1.729\n0.084\n-3.06e-05\n1.92e-06\n\n\nreview_scores_cleanliness\n0.1132\n0.001\n75.820\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0768\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0915\n0.002\n-50.902\n0.000\n-0.095\n-0.088\n\n\nroom_type_Private room\n-0.0145\n0.003\n-5.310\n0.000\n-0.020\n-0.009\n\n\nroom_type_Shared room\n-0.2519\n0.009\n-29.229\n0.000\n-0.269\n-0.235\n\n\ninstant_bookable_t\n0.3344\n0.003\n115.748\n0.000\n0.329\n0.340\n\n\n\n\n\nl_airbnb.summary() ```\n\n\n3. Interpretation of Results\nThe coefficients from the Poisson regression represent the log change in expected number of reviews per one-unit increase in each predictor. Exponentiating these values gives us approximate percentage changes. For example, the coefficient on instant_bookable_t is 0.3344, which implies that listings that are instantly bookable receive approximately 40% more reviews than those that are not ( 𝑒 0.3344 ≈ 1.40 e 0.3344 ≈1.40).\nSeveral listing characteristics are significantly associated with review counts. Listings with more bedrooms are associated with more reviews, while more bathrooms are unexpectedly associated with fewer. Cleanliness scores are strongly and positively related to review volume, while location and value scores are negatively associated — possibly reflecting competitive pressures in highly rated areas.\nRoom type also plays a significant role. Compared to entire homes/apartments, private rooms receive slightly fewer reviews, while shared rooms see a substantially lower expected review count. Price has a small negative coefficient and is only marginally significant.\nOverall, these results suggest that listing type, cleanliness, instant booking, and core features like bedrooms meaningfully affect how often a listing is booked (as proxied by reviews)."
  }
]