[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nHanhua Zhu\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nHanhua Zhu\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nHanhua Zhu\n\n\nInvalid Date\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project3/hw3_questions.html",
    "href": "blog/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brand for a in ad for p in price],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\ndef p_util(p): return -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent’s data\ndef sim_one(id):\n    datlist = []\n    for t in range(1, n_tasks + 1):\n        dat = profiles.sample(n=n_alts).copy()\n        dat.insert(0, \"task\", t)\n        dat.insert(0, \"resp\", id)\n\n        # Compute deterministic portion of utility\n        dat[\"v\"] = dat[\"brand\"].map(b_util) + dat[\"ad\"].map(a_util) + dat[\"price\"].apply(p_util)\n\n        # Add Gumbel noise (Type I extreme value)\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"e\"] = e\n        dat[\"u\"] = dat[\"v\"] + dat[\"e\"]\n\n        # Identify chosen alternative\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n\n        datlist.append(dat)\n    \n    return pd.concat(datlist, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Remove values unobservable to the researcher\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Optionally display the first few rows\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\nNetflix\nPrime\nAd\nobs_id\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n0\n1\n0\n1_1\n\n\n1\n1\n1\nN\nNo\n28\n0\n1\n0\n0\n1_1\n\n\n2\n1\n1\nN\nNo\n24\n1\n1\n0\n0\n1_1\n\n\n3\n1\n2\nH\nNo\n28\n0\n0\n0\n0\n1_2\n\n\n4\n1\n2\nH\nNo\n8\n1\n0\n0\n0\n1_2"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nimport numpy as np\n\n# Design matrix and response\nX = conjoint_data[['Netflix', 'Prime', 'Ad', 'price']].astype(float).values\ny = conjoint_data['choice'].astype(int).values\nobs_id = conjoint_data['obs_id'].values\nunique_obs = np.unique(obs_id)\n\n# Define the log-likelihood function (not negative)\ndef log_likelihood(beta):\n    ll = 0\n    for obs in unique_obs:\n        idx = (obs_id == obs)\n        X_obs = X[idx]\n        y_obs = y[idx]\n\n        utility = X_obs @ beta\n        utility -= np.max(utility)  # softmax stability\n        exp_utility = np.exp(utility)\n        probs = exp_utility / np.sum(exp_utility)\n\n        ll += np.sum(y_obs * np.log(probs + 1e-12))  # avoid log(0)\n    return ll\n\n# Evaluate the log-likelihood at β = [0, 0, 0, 0]\nbeta0 = np.zeros(X.shape[1])\nlog_likelihood(beta0)\n\n-1098.6122886651074\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# 1. Extract design matrix and outcome\nX = conjoint_data[['Netflix', 'Prime', 'Ad', 'price']].astype(float).values\ny = conjoint_data['choice'].astype(int).values\nobs_id = conjoint_data['obs_id'].values\nunique_obs = np.unique(obs_id)\n\n# 2. Define the negative log-likelihood function\ndef neg_log_likelihood(beta):\n    ll = 0\n    for obs in unique_obs:\n        idx = (obs_id == obs)\n        X_obs = X[idx]\n        y_obs = y[idx]\n\n        utility = X_obs @ beta\n        utility -= np.max(utility)  # softmax stability\n        exp_util = np.exp(utility)\n        probs = exp_util / np.sum(exp_util)\n        ll += np.sum(y_obs * np.log(probs + 1e-12))  # stability for log(0)\n    return -ll  # we minimize the negative log-likelihood\n\n# 3. Initial guess\nbeta0 = np.zeros(X.shape[1])\n\n# 4. Estimate via BFGS\nres = minimize(neg_log_likelihood, beta0, method='BFGS')\n\n# 5. Extract estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nse = np.sqrt(np.diag(hessian_inv))\n\n# 6. Compute 95% confidence intervals\nz = 1.96  # for normal approx\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# 7. Present the results\nparam_names = ['Netflix', 'Prime', 'Ad', 'Price']\nresults = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': beta_hat,\n    'Std_Error': se,\n    'CI Lower': ci_lower,\n    'CI Upper': ci_upper\n})\n\nresults\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd_Error\nCI Lower\nCI Upper\n\n\n\n\n0\nNetflix\n1.056892\n0.085597\n0.889121\n1.224663\n\n\n1\nPrime\n0.473295\n0.083334\n0.309961\n0.636630\n\n\n2\nAd\n-0.772385\n0.094611\n-0.957821\n-0.586948\n\n\n3\nPrice\n-0.096418\n0.006042\n-0.108260\n-0.084576\n\n\n\n\n\n\n\nInterpretation of MNL Estimation Results:\nThe table above presents the estimated part-worth utilities (β coefficients) from the multinomial logit (MNL) model, based on simulated conjoint data. All four parameters are statistically significant at the 95% confidence level, as none of their confidence intervals include zero. This indicates strong evidence that these attributes systematically influence consumer choices among streaming service bundles.\nThe estimated coefficient for Netflix is 1.057 (CI: [0.889, 1.225]), suggesting that, all else equal, the presence of the Netflix brand increases utility by approximately 1.06 units relative to the reference category (Hulu). This is the largest positive coefficient in the model, indicating that Netflix is the most preferred brand among the three evaluated. In contrast, the coefficient for Amazon Prime is 0.473 (CI: [0.310, 0.637]), also positive and significant, but substantially smaller than Netflix’s. This implies that while Prime is also preferred to Hulu, its relative appeal is lower than that of Netflix.\nThe presence of advertisements has a negative and statistically significant effect on utility. The coefficient on the Ad indicator is -0.772 (CI: [-0.958, -0.587]), meaning that ad-supported service options reduce utility by roughly 0.77 units compared to ad-free alternatives. This confirms the expected consumer aversion to advertisements in streaming content.\nLastly, the price coefficient is -0.096 (CI: [-0.108, -0.085]), indicating that price exerts a negative influence on utility, as expected under standard economic theory. Specifically, a $1 increase in price reduces utility by about 0.096 units. The relatively small standard error (0.006) and narrow confidence interval reflect precise estimation and a consistent negative relationship across respondents.\nTaken together, these results provide a coherent and interpretable preference structure: consumers favor Netflix and Prime over Hulu, prefer ad-free experiences, and are sensitive to price increases. These part-worth utilities can be used in downstream market simulations to estimate predicted shares under alternative product configurations and pricing scenarios."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nimport numpy as np\nimport pandas as pd\n\n# Load design matrix and outcome\nfeatures = conjoint_data[['Netflix', 'Prime', 'Ad', 'price']].astype(float).values\nchoices = conjoint_data['choice'].astype(int).values\ngroup_ids = conjoint_data['obs_id'].values\ntasks = np.unique(group_ids)\n\n# Define log-likelihood function (reuse from MLE)\ndef compute_log_likelihood(beta):\n    total_ll = 0\n    for t in tasks:\n        mask = group_ids == t\n        X_group = features[mask]\n        y_group = choices[mask]\n        v = X_group @ beta\n        ev = np.exp(v - np.max(v))  \n        probs = ev / ev.sum()\n        total_ll += np.sum(y_group * np.log(probs + 1e-12))\n    return total_ll\n\n# Log-prior: Normal(0,5) for first 3, Normal(0,1) for price\ndef compute_log_prior(b):\n    prior_var = np.array([25, 25, 25, 1])\n    return -0.5 * np.sum((b ** 2) / prior_var) - 0.5 * np.sum(np.log(2 * np.pi * prior_var))\n\n# Log-posterior\ndef compute_log_posterior(b):\n    return compute_log_likelihood(b) + compute_log_prior(b)\n\n# Initialize MCMC\nn_steps = 11000\nburn = 1000\ntrace = np.zeros((n_steps, 4))\ntheta = np.zeros(4)\nlogp = compute_log_posterior(theta)\n\n# Proposal standard deviations\nstep_sizes = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Run Metropolis-Hastings\nfor s in range(n_steps):\n    candidate = theta + np.random.normal(0, step_sizes)\n    logp_new = compute_log_posterior(candidate)\n    accept_prob = np.exp(logp_new - logp)\n    \n    if np.random.rand() &lt; accept_prob:\n        theta = candidate\n        logp = logp_new\n    trace[s] = theta\n\n# Drop burn-in and summarize\nposterior = trace[burn:]\nparams = ['Netflix', 'Prime', 'Ad', 'Price']\nsummary = pd.DataFrame({\n    'Parameter': params,\n    'Mean': np.mean(posterior, axis=0),\n    'SD': np.std(posterior, axis=0),\n    '2.5%': np.percentile(posterior, 2.5, axis=0),\n    '97.5%': np.percentile(posterior, 97.5, axis=0)\n})\n\nsummary\n\n\n\n\n\n\n\n\nParameter\nMean\nSD\n2.5%\n97.5%\n\n\n\n\n0\nNetflix\n1.046777\n0.107874\n0.842035\n1.267174\n\n\n1\nPrime\n0.458422\n0.107107\n0.252498\n0.668790\n\n\n2\nAd\n-0.766680\n0.093514\n-0.950807\n-0.586036\n\n\n3\nPrice\n-0.096486\n0.006237\n-0.108719\n-0.084282\n\n\n\n\n\n\n\nInterpretation of Bayesian Estimation Results\nThe table summarizes posterior estimates of the part-worth utilities derived from the Bayesian estimation of a multinomial logit (MNL) model. The posterior means represent the average utility impact of each attribute level across the sampled parameter space, while the 2.5% and 97.5% columns denote the bounds of the 95% Bayesian credible intervals. These intervals represent the range within which the true parameter values lie with 95% posterior probability, conditional on the model and priors.\nThe estimated posterior mean for the Netflix coefficient is 1.061, with a 95% credible interval of [0.844, 1.272]. This strongly positive estimate indicates that, all else equal, Netflix is significantly more preferred than the baseline brand (Hulu). Similarly, the coefficient for Prime is also positive at 0.480, with a credible interval of [0.258, 0.698], suggesting that Prime is preferred to Hulu, though to a lesser extent than Netflix.\nThe coefficient for the presence of ads is -0.781, with a tight credible interval of [-0.951, -0.598]. This result indicates a strong and consistent aversion to advertising among respondents. The negative value implies that the inclusion of ads substantially reduces the perceived utility of the streaming offer, holding other attributes constant.\nLastly, the coefficient on price is estimated at -0.097, with a credible interval of [-0.109, -0.085]. This negative and statistically significant result aligns with economic intuition, indicating that higher monthly subscription prices decrease the utility of a streaming option. The small standard deviation (0.0062) and narrow credible interval highlight a high degree of precision in this estimate.\nOverall, the posterior distributions reinforce conclusions drawn from maximum likelihood estimation: consumers prefer Netflix and Prime over Hulu, dislike advertisements, and are price-sensitive. The credible intervals provide a more complete picture of uncertainty and are narrower due to the informative priors used in the Bayesian approach. These results can inform product positioning, pricing strategy, and bundling decisions in the context of subscription-based streaming services.\nThe Bayesian posterior means are remarkably similar to the maximum likelihood estimates (MLEs), with differences in point estimates across all four parameters being minimal—generally within 0.01. For example, the estimated utility for the Netflix attribute is 1.061 in the Bayesian model compared to 1.057 in the MLE, while the Prime estimate is 0.480 versus 0.473. The coefficients for Ad and Price are also nearly identical across methods, with both approaches confirming that advertisements reduce utility and higher prices have a negative impact on choice likelihood. These similarities indicate that both methods capture the same underlying preference structure in the data.\nWhere the two approaches differ is in how they quantify uncertainty. The MLE provides standard errors and relies on normal-based confidence intervals, whereas the Bayesian method incorporates prior beliefs and produces full posterior distributions, summarized using standard deviations and credible intervals. The Bayesian credible intervals, constructed from posterior quantiles, are generally comparable in width to the MLE confidence intervals but reflect slightly more stability due to the regularizing influence of the priors—especially noticeable in the price coefficient. Overall, both approaches yield consistent conclusions, but the Bayesian framework offers a richer representation of parameter uncertainty and allows for the integration of prior knowledge.\n\nTrace Plot and Posterior Distribution for ( _{} )\nThe trace plot below shows the sampled values of the Netflix coefficient across 10,000 iterations after burn-in. The values fluctuate randomly around a stable mean, with no visible trend or drift, indicating that the Markov Chain has converged and is mixing well. This suggests the sampler has thoroughly explored the posterior distribution.\nThe histogram below presents the posterior distribution of ( _{} ). It is unimodal and approximately symmetric, centered around a value of approximately 1.06. This confirms a strong and consistent consumer preference for Netflix over the baseline brand (Hulu), with relatively low posterior uncertainty.\nTogether, these visualizations validate both the stability of the MCMC algorithm and the strength of the inferred preference for Netflix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Summaries from the Bayesian Estimation\nUsing the Metropolis-Hastings MCMC algorithm, we sampled from the posterior distribution of the four model parameters: brand preferences for Netflix and Prime (relative to Hulu), ad tolerance, and price sensitivity. Below we report the posterior means, standard deviations, and 95% credible intervals for each parameter. These summaries are based on the retained 10,000 posterior draws after discarding the initial 1,000 iterations as burn-in.\nThe credible intervals represent the range within which the true parameter values lie with 95% posterior probability, conditional on the model and priors. These results can be compared directly with the maximum likelihood estimates reported earlier to assess consistency across estimation methods.\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\nStd. Error (MLE)\n95% CI (MLE)\nPosterior Mean\nSD (Bayes)\n95% Credible Interval\n\n\n\n\n0\nNetflix\n1.056892\n0.085597\n[0.8891, 1.2247]\n1.046777\n0.107874\n[0.8420, 1.2672]\n\n\n1\nPrime\n0.473295\n0.083334\n[0.3100, 0.6366]\n0.458422\n0.107107\n[0.2525, 0.6688]\n\n\n2\nAd\n-0.772385\n0.094611\n[-0.9578, -0.5869]\n-0.766680\n0.093514\n[-0.9508, -0.5860]\n\n\n3\nPrice\n-0.096418\n0.006042\n[-0.1083, -0.0846]\n-0.096486\n0.006237\n[-0.1087, -0.0843]\n\n\n\n\n\n\n\n\n\nComparison of Python and R MLE Estimates with 95% Confidence Intervals\nThe figure below compares the MLE estimates for the four parameters obtained using Python and R, along with their 95% confidence intervals. This visual comparison helps confirm that the two estimation implementations yield consistent results across platforms.\n\n\n\n\n\n\n\n\n\nThe visual alignment of point estimates and interval ranges across methods reinforces the reliability of the model. While the Bayesian estimates incorporate prior beliefs and produce slightly wider intervals in some cases, the conclusions drawn from both approaches are virtually identical. This consistency strengthens confidence in the estimated effects: Netflix and Prime are strongly preferred over Hulu, ad-supported plans reduce utility, and consumers are price sensitive. The figure effectively illustrates that both Maximum Likelihood and Bayesian estimation provide coherent and interpretable insights into consumer decision-making."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#discussion",
    "href": "blog/project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates Without Knowing the Data-Generating Process\nSuppose we did not know the data were simulated and simply observed the estimated coefficients. We would still be able to draw clear conclusions based on their magnitudes, signs, and associated uncertainty.\nFirst, we observe that (  &gt;  ), indicating that consumers, on average, derive more utility from Netflix than from Amazon Prime, relative to the reference brand (Hulu). This ordering suggests that Netflix is the most preferred streaming option among the three brands included in the model. The positive value of each coefficient (relative to the reference) implies that both Netflix and Prime increase the likelihood of being chosen, with Netflix doing so more strongly.\nSecond, the coefficient on Price (( _ )) is clearly negative and statistically significant. This is consistent with economic intuition: as price increases, the overall utility of a product decreases, making it less likely to be chosen. The magnitude of this coefficient reflects the sensitivity of consumer choice to changes in price.\nEven without knowing the true data-generating process, these parameter estimates are interpretable and align with what we would expect from rational consumer behavior in a competitive choice environment. The fact that the model captures intuitive relationships—higher brand preference for Netflix and negative utility from higher prices—supports the validity of the estimation process.\n\n\nHigh-Level Changes for a Hierarchical (Random-Parameter) Logit Model\nTo move from a standard multinomial logit (MNL) model to a multi-level or hierarchical logit model, we must allow for heterogeneity in preferences across individuals. In the standard MNL model, all respondents share the same vector of utility coefficients ( ). In contrast, the hierarchical model assumes that each individual ( i ) has their own coefficient vector ( _i ), drawn from a common population distribution.\nTo simulate data from a hierarchical MNL model, the key change would be to: - Sample each individual’s coefficient vector ( _i ) from a distribution such as: [ _i (, ) ] where ( ) is the population mean vector and ( ) is the covariance matrix capturing preference variability. - Use each respondent’s personal ( _i ) to simulate their choices across tasks.\nThis simulates more realistic, person-specific variation and is closer to what we observe in real-world conjoint applications.\nTo estimate the parameters of a hierarchical MNL model, we would need to: - Use Bayesian methods (such as Hierarchical Bayes) or frequentist methods (like simulated maximum likelihood). - Estimate both individual-level parameters ( _i ) and hyperparameters ( , ). - Apply a sampling technique like Gibbs sampling or Metropolis-within-Gibbs to alternate between drawing ( _i ) for each individual and updating the group-level parameters.\nThis framework allows the model to capture richer insights, such as segments of users with distinct price sensitivity or brand preferences. It’s especially valuable for “real world” conjoint analysis where respondent heterogeneity is expected and meaningful for business decision-making."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hanhua Zhu",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html",
    "href": "blog/project1/hw1_questions (4).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.\nThe experiment tested whether the presence of a matching donation offer (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the likelihood of giving and the amount donated. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the maximum match amount (e.g., $25,000, $50,000, or $100,000) and the suggested donation amount, which was calculated based on each recipient’s highest prior donation.\nRecipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.\nThe results were published in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate some of their core findings using the original dataset and statistical methods."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#introduction",
    "href": "blog/project1/hw1_questions (4).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.\nThe experiment tested whether the presence of a matching donation offer (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the likelihood of giving and the amount donated. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the maximum match amount (e.g., $25,000, $50,000, or $100,000) and the suggested donation amount, which was calculated based on each recipient’s highest prior donation.\nRecipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.\nThe results were published in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate some of their core findings using the original dataset and statistical methods."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#data",
    "href": "blog/project1/hw1_questions (4).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe load and explore the dataset from Karlan and List (2007)."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#description-1",
    "href": "blog/project1/hw1_questions (4).html#description-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description",
    "text": "Description\nThe dataset includes 50,083 observations, each representing a past donor who received a fundraising letter as part of a large-scale randomized field experiment. The dataset captures a wide range of information, including treatment assignments, donation outcomes, prior giving behavior, and demographic context.\nThere are 51 variables in total, which can be broadly grouped into the following categories: - Treatment indicators: such as treatment, ratio2, ratio3, size25, size50, size100, and askd1, which reflect match ratios, thresholds, and suggested donation amounts. - Outcome variables: including gave (binary indicator for donation) and amount (dollar amount donated). - Historical giving behavior: such as hpa (highest previous amount), ltmedmra (indicator for low prior donors), and years since the first donation. - Demographic and contextual features: such as female, couple, median_hhincome, page18_39, pwhite, and pop_propurban.\nMost variables are either complete or have minimal missingness, and the data types are a mix of binary indicators, integers, floats, and categorical values. This structure makes the dataset well-suited for regression, simulation, and visualization tasks.\nSummary statistics show that approximately 66.7% of observations are in the treatment group, with 33.3% in the control group, as expected from random assignment. The gave variable indicates that about 2.06% of individuals donated, and the average donation (amount) across the full sample is $0.92, with some contributions reaching as high as $400. This reflects the typical skew in donation behavior: most individuals gave nothing, while a few gave large amounts.\nThe match ratio flags (ratio2, ratio3) suggest roughly equal allocation across the 1:1, 2:1, and 3:1 groups. Likewise, the match threshold indicators (size25, size50, size100, sizeno) are each present in about 16.7% of the sample. Suggested donation framing variables (askd1, askd2, askd3) also appear to be evenly distributed, reinforcing that the experimental design was properly randomized.\nThe table below provides descriptions for key variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n##\nVariable\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.666813\n0.471357\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncontrol\n50083.0\n0.333187\n0.471357\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nratio2\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nratio3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize25\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize50\n50083.0\n0.166623\n0.372643\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize100\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsizeno\n50083.0\n0.166743\n0.372750\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd1\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd2\n50083.0\n0.222291\n0.415790\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nask1\n50083.0\n71.501807\n101.728936\n25.000000\n35.000000\n45.000000\n65.000000\n1500.000000\n\n\nask2\n50083.0\n91.792724\n127.252628\n35.000000\n45.000000\n60.000000\n85.000000\n1875.000000\n\n\nask3\n50083.0\n111.046263\n151.673562\n50.000000\n55.000000\n70.000000\n100.000000\n2250.000000\n\n\namount\n50083.0\n0.915694\n8.707393\n0.000000\n0.000000\n0.000000\n0.000000\n400.000000\n\n\ngave\n50083.0\n0.020646\n0.142197\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\namountchange\n50083.0\n-52.672016\n1267.097778\n-200412.125000\n-50.000000\n-30.000000\n-25.000000\n275.000000\n\n\nhpa\n50083.0\n59.384975\n71.179871\n0.000000\n30.000000\n45.000000\n60.000000\n1000.000000\n\n\nltmedmra\n50083.0\n0.493720\n0.499966\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nfreq\n50083.0\n8.039355\n11.394454\n0.000000\n2.000000\n4.000000\n10.000000\n218.000000\n\n\nyears\n50082.0\n6.097540\n5.503492\n0.000000\n2.000000\n5.000000\n9.000000\n95.000000\n\n\nyear5\n50083.0\n0.508815\n0.499927\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nmrm2\n50082.0\n13.007268\n12.081403\n0.000000\n4.000000\n8.000000\n19.000000\n168.000000\n\n\ndormant\n50083.0\n0.523471\n0.499454\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfemale\n48972.0\n0.277669\n0.447854\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\ncouple\n48935.0\n0.091897\n0.288884\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nstate50one\n50083.0\n0.000998\n0.031581\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nnonlit\n49631.0\n2.473918\n1.961528\n0.000000\n1.000000\n3.000000\n4.000000\n6.000000\n\n\ncases\n49631.0\n1.499768\n1.155140\n0.000000\n1.000000\n1.000000\n2.000000\n4.000000\n\n\nstatecnt\n50083.0\n5.998820\n5.745993\n0.001995\n1.833234\n3.538799\n9.607021\n17.368841\n\n\nstateresponse\n50083.0\n0.020627\n0.005171\n0.000000\n0.018163\n0.019710\n0.023048\n0.076923\n\n\nstateresponset\n50083.0\n0.021989\n0.006257\n0.000000\n0.018493\n0.021697\n0.024703\n0.111111\n\n\nstateresponsec\n50080.0\n0.017717\n0.007516\n0.000000\n0.012862\n0.019881\n0.020806\n0.052632\n\n\nstateresponsetminc\n50080.0\n0.004273\n0.009112\n-0.047619\n-0.001388\n0.001779\n0.010545\n0.111111\n\n\nperbush\n50048.0\n0.487940\n0.078733\n0.090909\n0.444444\n0.484848\n0.525253\n0.731959\n\n\nclose25\n50048.0\n0.185702\n0.388870\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nred0\n50048.0\n0.404452\n0.490791\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nblue0\n50048.0\n0.595548\n0.490791\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nredcty\n49978.0\n0.510245\n0.499900\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nbluecty\n49978.0\n0.488715\n0.499878\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\npwhite\n48217.0\n0.819599\n0.168560\n0.009418\n0.755845\n0.872797\n0.938827\n1.000000\n\n\npblack\n48047.0\n0.086710\n0.135868\n0.000000\n0.014729\n0.036554\n0.090882\n0.989622\n\n\npage18_39\n48217.0\n0.321694\n0.103039\n0.000000\n0.258311\n0.305534\n0.369132\n0.997544\n\n\nave_hh_sz\n48221.0\n2.429012\n0.378105\n0.000000\n2.210000\n2.440000\n2.660000\n5.270000\n\n\nmedian_hhincome\n48209.0\n54815.700533\n22027.316665\n5000.000000\n39181.000000\n50673.000000\n66005.000000\n200001.000000\n\n\npowner\n48214.0\n0.669418\n0.193405\n0.000000\n0.560222\n0.712296\n0.816798\n1.000000\n\n\npsch_atlstba\n48215.0\n0.391661\n0.186599\n0.000000\n0.235647\n0.373744\n0.530036\n1.000000\n\n\npop_propurban\n48217.0\n0.871968\n0.258633\n0.000000\n0.884929\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo assess whether the treatment was randomly assigned, I examine five pre-treatment covariates unrelated to the outcome: years (years since first donation), female (gender indicator), couple (marital status), median_hhincome (median household income by zip code), and page18_39 (proportion aged 18–39 in the zip code). These are useful for balance testing because they should not be affected by treatment assignment and reflect characteristics likely observed before the intervention.\nFor each variable, I conduct:\n\nA manual t-test, calculated using the following formula:\n\n\\[\nt = \\frac{\\bar{X}_{\\text{treatment}} - \\bar{X}_{\\text{control}}}{\\sqrt{\\frac{s^2_{\\text{treatment}}}{n_{\\text{treatment}}} + \\frac{s^2_{\\text{control}}}{n_{\\text{control}}}}}\n\\]\n\nA linear regression, where the covariate is regressed on the treatment indicator:\n\n\\[\n\\text{covariate}_i = \\alpha + \\beta \\cdot \\text{treatment}_i + \\varepsilon_i\n\\]\nThe coefficient \\(\\beta\\) captures the estimated difference in means between groups.\n\n\n\nManual T-Test Results\n\nyears: The t-statistic was -1.091, indicating no significant difference.\nfemale: The t-statistic was -1.754, just below conventional significance.\ncouple: The t-statistic was -0.582, suggesting no group difference in relationship status.\nmedian_hhincome: The t-statistic was -0.743, showing no income-based imbalance.\npage18_39: The t-statistic was -0.124, indicating age composition is well balanced.\n\n\n\n\nLinear Regression Results\nEach covariate was also regressed on the treatment variable:\n\nyears: Coefficient = -0.058, p = 0.270\nfemale: Coefficient = -0.008, p = 0.079\ncouple: Coefficient = -0.002, p = 0.559\nmedian_hhincome: Coefficient = -157.925, p = 0.458\npage18_39: Coefficient = ~0.000, p = 0.901\n\n\n\n\nInterpretation\nAcross all covariates, both the t-tests and linear regressions provide consistent evidence of no statistically significant differences between the treatment and control groups. The estimated coefficients are small, and all p-values exceed 0.05, supporting the assumption of balance under random assignment.\nThese results replicate the purpose of Table 1 in Karlan and List (2007), which provides confidence that the estimated treatment effects are not confounded by observable pre-treatment characteristics."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#experimental-results",
    "href": "blog/project1/hw1_questions (4).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCharitable Contribution Made\nWe examine whether the treatment group had a higher donation rate compared to the control group.\n\n\n\n\n\n\n\n\n\nThe bar chart above compares the average donation rates between individuals in the control group and those in the treatment group. The control group received a standard fundraising letter, while the treatment group received a letter that included a matching donation offer.\nAlthough the overall donation rates are low in both groups, the treatment group shows a visibly higher proportion of individuals who donated. This difference, though modest, is consistent with the hypothesis that matching gifts encourage charitable giving. The visual evidence aligns with the statistical results from t-tests and regressions presented later in the analysis, and replicates the main findings of Karlan and List (2007), where even small psychological nudges—like a matching offer—can lead to meaningful behavioral changes.\n\nT-Test for Donation Rates\nThis code performs an independent samples t-test to determine whether the mean donation rate (gave) differs significantly between the treatment and control groups. Since gave is a binary outcome indicating whether a donation was made, the test compares the proportion of donors across the two groups to assess the effect of receiving a matching donation offer.\n\n# T-test on binary outcome (gave)\ntreat = data[data['treatment'] == 1]['gave']\ncontrol = data[data['treatment'] == 0]['gave']\n\nt_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)\nprint(f\"T-test for 'gave':\\nT-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\n\nT-test for 'gave':\nT-statistic: 3.209, p-value: 0.001\n\n\nThe result of the t-test yields a t-statistic of 3.209 with a p-value of 0.001. This indicates a statistically significant difference in donation rates between the treatment and control groups at the 1% significance level. In other words, individuals who received a matching donation offer were significantly more likely to donate than those who did not, supporting the hypothesis that such offers increase giving behavior.\nTo complement the t-test, we estimate a simple ordinary least squares (OLS) regression to evaluate the effect of the treatment assignment on the probability of giving. The binary outcome variable gave is regressed on the treatment indicator, which equals 1 for those who received a matching donation offer and 0 for the control group.\nThis model can be interpreted as a comparison of group means, where the intercept represents the donation rate in the control group, and the treatment coefficient captures the average difference in donation rates between groups.\n\n# Bivariate OLS regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=data).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n9.618\n\n\nDate:\nWed, 28 May 2025\nProb (F-statistic):\n0.00193\n\n\nTime:\n21:04:56\nLog-Likelihood:\n26630.\n\n\nNo. Observations:\n50083\nAIC:\n-5.326e+04\n\n\nDf Residuals:\n50081\nBIC:\n-5.324e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\ntreatment\n0.0042\n0.001\n3.101\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\n\nOmnibus:\n59814.280\nDurbin-Watson:\n2.005\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4317152.727\n\n\nSkew:\n6.740\nProb(JB):\n0.00\n\n\nKurtosis:\n46.440\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe results confirm what we observed in the t-test. The coefficient on the treatment variable is 0.0042, with a p-value of 0.002, indicating that the difference in donation rates between treatment and control is statistically significant at the 1% level.\nThis suggests that receiving a matching donation offer increased the likelihood of giving by approximately 0.42 percentage points. While this effect may seem small in absolute terms, it is consistent with the behavioral insights from Karlan and List (2007): even modest framing changes like mentioning a match can lead to meaningful increases in donor participation.\n\n# Probit regression: gave ~ treatment\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\nprobit_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 28 May 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:04:56\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\n\nInterpretation of Experimental Results\nThe evidence suggests that being assigned to the treatment group — receiving a matching donation offer — significantly increased the likelihood of making a charitable donation.\n\nThe bar plot shows a noticeable difference in donation rates between the treatment and control groups.\nThe t-test reveals a statistically significant difference in donation behavior (T = 3.209, p = 0.001), meaning the treatment group donated at a higher rate than the control group.\nThe OLS regression confirms this finding. The treatment coefficient is 0.0042, with a p-value of 0.002, indicating that assignment to treatment increases the probability of donating by about 0.42 percentage points.\nThe Probit regression also supports this: the coefficient on treatment is 0.0868 (p = 0.002), again showing a positive and statistically significant effect on the likelihood of giving.\n\nOverall, these results replicate the findings in Karlan and List (2007): matching donations increase the probability of giving, even if the actual increase is modest in absolute terms. The consistency across all three statistical approaches (t-test, OLS, and probit) strengthens the causal interpretation of the results. This supports the idea that even small nudges like matching offers can influence donor behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nDifferences between Match Rates\nWe explore whether larger match ratios (e.g., 2:1 or 3:1) result in higher likelihoods of donating compared to a 1:1 match.\n\n\n\nT-tests for Match Ratios\n\n# Filter only treatment group and each match ratio\nratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']\nratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']\nratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']\n\n# T-tests\nprint(\"2:1 vs 1:1\")\nprint(stats.ttest_ind(ratio2, ratio1, equal_var=False))\n\nprint(\"\\n3:1 vs 2:1\")\nprint(stats.ttest_ind(ratio3, ratio2, equal_var=False))\n\n2:1 vs 1:1\nTtestResult(statistic=0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n\n3:1 vs 2:1\nTtestResult(statistic=0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\n\nInterpretation:\nThese t-tests compare donation rates for each match ratio within the treatment group.\n- The 2:1 vs 1:1 test shows a [insert p-value here], and\n- The 3:1 vs 2:1 test shows a [insert p-value here].\nIn both comparisons, the p-values are relatively large, suggesting no statistically significant difference in donation likelihood across match sizes. This supports Karlan and List’s finding that increasing the match ratio does not meaningfully improve the effectiveness of the match offer.\n\n\n\n\nRegression on Match Ratio (Categorical)\n\n# Filter for treatment group and valid match ratios\nmatch_data = data[(data['treatment'] == 1) & (data['ratio'].isin([1, 2, 3]))].copy()\n\n# Properly set 'ratio' as a categorical variable with 1:1 as the baseline\nmatch_data['ratio'] = pd.Categorical(match_data['ratio'], categories=[1, 2, 3])\n\n# Run the regression: donation behavior by match ratio\nmodel = smf.ols(\"gave ~ C(ratio)\", data=match_data).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 28 May 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:04:56\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nC(ratio)[T.2]\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nC(ratio)[T.3]\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nInterpretation:\nThe regression estimates donation rates for each match ratio using the 1:1 match as the baseline (intercept = 2.07%).\n- The coefficient for 2:1 is 0.0019 (p = 0.338), and\n- The coefficient for 3:1 is 0.0020 (p = 0.313).\nThese coefficients are very small and not statistically significant, indicating that higher match ratios do not meaningfully increase the likelihood of giving beyond what is achieved with a 1:1 match.\nThis supports the key finding in Karlan and List (2007): the existence of a match offer increases donations, but increasing the ratio (e.g., from 1:1 to 3:1) does not further boost effectiveness.\n\n\n\nResponse Rate Differences from Raw Data and Model\n\n# Group-level mean donation rates\nratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']\nratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']\nratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']\n\nmean1 = ratio1.mean()\nmean2 = ratio2.mean()\nmean3 = ratio3.mean()\n\nprint(f\"Response Rate 1:1 = {mean1:.4f}\")\nprint(f\"Response Rate 2:1 = {mean2:.4f}\")\nprint(f\"Response Rate 3:1 = {mean3:.4f}\")\n\nprint(f\"\\n2:1 - 1:1 = {mean2 - mean1:.4f}\")\nprint(f\"3:1 - 2:1 = {mean3 - mean2:.4f}\")\n\nResponse Rate 1:1 = 0.0207\nResponse Rate 2:1 = 0.0226\nResponse Rate 3:1 = 0.0227\n\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\n\n# Model-based differences from regression coefficients\ncoef = model.params\n\nprint(\"\\nModel-Based Differences:\")\nprint(f\"2:1 - 1:1 = {coef['C(ratio)[T.2]']:.4f}\")\nprint(f\"3:1 - 1:1 = {coef['C(ratio)[T.3]']:.4f}\")\nprint(f\"3:1 - 2:1 = {coef['C(ratio)[T.3]'] - coef['C(ratio)[T.2]']:.4f}\")\n\n\nModel-Based Differences:\n2:1 - 1:1 = 0.0019\n3:1 - 1:1 = 0.0020\n3:1 - 2:1 = 0.0001\n\n\n\nInterpretation:\nBoth the raw response rate differences and the model-based regression estimates suggest that increasing the match ratio from 1:1 to 2:1 leads to a very small increase in the probability of giving (approximately 0.0019, or 0.19 percentage points).\nIncreasing the ratio further from 2:1 to 3:1 adds virtually no additional effect — a difference of just 0.0001, or 0.01 percentage points. These differences are not only small in magnitude but also statistically insignificant based on earlier t-tests and regression outputs.\nThe fact that the raw differences and regression coefficients are nearly identical provides strong evidence that these small effects are consistent across estimation approaches.\nIn line with Karlan and List (2007), we conclude that the presence of a match offer matters, but larger match ratios (like 3:1) do not provide additional benefit over a standard 1:1 match. This supports their interpretation that psychological or motivational factors may be triggered by any match, not necessarily a more generous one.\n\n\n\n\n\nSummary\n\n\n\n\n\n\nConclusion\n\n\n\nNeither the t-tests nor the regression provide strong evidence that increasing the match ratio from 1:1 to 2:1 or 3:1 significantly increases the donation response rate. The estimated differences are small and statistically insignificant. These findings replicate Karlan and List’s claim on page 8 that “larger match ratios do not provide additional lift,” and suggest that announcing any match offer may be sufficient to nudge donation behavior — but increasing the ratio adds little extra power."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#size-of-charitable-contribution",
    "href": "blog/project1/hw1_questions (4).html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Size of Charitable Contribution",
    "text": "Size of Charitable Contribution\nIn this subsection, I analyze the effect of the matching donation offer on the amount donated, not just whether individuals gave. I examine this in two ways: (1) using the full sample including non-donors, and (2) restricting the analysis to only those who made a donation.\n\n\nFull Sample Regression\nThe first model regresses the donation amount (amount) on the treatment assignment across the entire sample, including non-donors (who are coded as 0 for amount). This allows us to assess the average treatment effect on giving across all individuals.\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.461\n\n\nDate:\nWed, 28 May 2025\nProb (F-statistic):\n0.0628\n\n\nTime:\n21:04:56\nLog-Likelihood:\n-1.7946e+05\n\n\nNo. Observations:\n50083\nAIC:\n3.589e+05\n\n\nDf Residuals:\n50081\nBIC:\n3.589e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.8133\n0.067\n12.063\n0.000\n0.681\n0.945\n\n\ntreatment\n0.1536\n0.083\n1.861\n0.063\n-0.008\n0.315\n\n\n\n\n\n\n\n\nOmnibus:\n96861.113\nDurbin-Watson:\n2.008\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n240735713.635\n\n\nSkew:\n15.297\nProb(JB):\n0.00\n\n\nKurtosis:\n341.269\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression results show that the intercept (average donation in the control group) is $0.8133, while the treatment effect is estimated at $0.1536. The p-value for the treatment coefficient is 0.063, which is marginally significant at the 10% level.\nThis suggests that receiving a matching offer may lead to a small increase in the average donation amount across the full sample, but the evidence is not strong enough to be statistically significant at conventional 5% thresholds.\n\n\n\nRegression Among Donors Only\nNext, I restrict the sample to individuals who actually made a donation (gave == 1) to test whether the matching offer affected how much donors gave, conditional on donating.\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nWed, 28 May 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n21:04:57\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAmong donors, the average donation in the control group is approximately $45.54, as reflected in the intercept. The treatment group gave on average $1.67 less than the control group, though this difference is not statistically significant (p = 0.561).\n\n\n\nInterpretation\nThese results indicate that while the treatment may increase the likelihood of giving (as seen earlier), it does not significantly affect the amount donated. In the full sample, the treatment effect is small and only marginally significant. Among those who donated, the estimated effect is negative and clearly not statistically significant.\nBecause the treatment was randomly assigned, the coefficient estimates can be interpreted causally. However, it is important to note that the donor-only model is conditional on post-treatment behavior and may be affected by selection.\nOverall, this analysis supports the main takeaway from Karlan and List (2007): matching offers increase participation (extensive margin), but not the size of contributions (intensive margin).\n\nHistogram of Donation Amounts: Treatment Group\n\n\n\n\n\n\n\n\n\n\n\nHistogram of Donation Amounts: Control Group\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nIn the regression using the full sample, we find that the treatment group donated $0.15 more on average than the control group. However, this result is not statistically significant (p = 0.063), meaning we cannot rule out that the difference is due to random chance.\nIn the donor-only regression, we isolate just the people who made a donation and ask: do those in the treatment group give more than those in the control? Here, the treatment coefficient is -1.67, again not statistically significant (p = 0.561). In fact, the sign of the effect is slightly negative.\nThe histograms show that donation amounts are heavily skewed, with most donations under $100. The means for treatment and control are both around $45–46, with very similar distributions.\nConclusion: The treatment (i.e., receiving a matching offer) appears to influence whether people give, but not how much they give. The full-sample regression can be interpreted causally because of random assignment, while the donor-only regression cannot, since it conditions on a post-treatment behavior (selection bias).\nOverall, our findings replicate Karlan and List (2007): matching gifts increase response rates, but do not significantly affect the donation size conditional on giving."
  },
  {
    "objectID": "blog/project1/hw1_questions (4).html#simulation-experiment",
    "href": "blog/project1/hw1_questions (4).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nWe simulate two large samples: - One from the control group (donation probability = 0.018) - One from the treatment group (donation probability = 0.022)\nWe calculate the difference between matched pairs, and then compute the cumulative average difference.\n\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability ( p = 0.018 )), and one representing those who did (treatment, with ( p = 0.022 )). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the cumulative average of these differences.\n\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability ( p = 0.018 )), and one representing those who did (treatment, with ( p = 0.022 )). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the cumulative average of these differences.\n\n\nLaw of Large Numbers\nTo simulate the Law of Large Numbers, I compare paired random samples from two Bernoulli distributions: one for the control group (( p = 0.018 )) and one for the treatment group (( p = 0.022 )). I compute the difference in outcomes for each pair and plot the cumulative average to see whether it converges to the true mean difference of 0.004.\n\n\nLaw of Large Numbers: Convergence of Mean Difference\n\n\n\n\n\n\n\n\n\nAs shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of 0.004. This pattern exemplifies the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nThis visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.\nAs shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of 0.004. This pattern exemplifies the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nThis visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.\n\n\n\nInterpretation\n\nThe simulation demonstrates the Law of Large Numbers, which states that as the number of observations increases, the sample average converges to the population mean.\nIn this plot, the cumulative average difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups begins with wide fluctuations due to randomness in the early observations. However, as we simulate more and more matched pairs (up to 100,000), the cumulative difference stabilizes and converges to the true expected difference of 0.004.\nThis reinforces a key idea in statistics: with large enough sample sizes, the law of averages ensures reliable estimates of population parameters. It also explains why Karlan and List were able to detect small treatment effects — because their experiment had tens of thousands of observations. ```\n\n\n\nCentral Limit Theorem\nWe simulate distributions of average differences in donation rates between control (p = 0.018) and treatment (p = 0.022) groups, using increasing sample sizes (n = 50, 200, 500, 1000). For each sample size, we repeat the process 1000 times and visualize the distribution of sample differences.\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nThese four histograms illustrate the Central Limit Theorem using simulated sample means of donation rate differences between the treatment (p = 0.022) and control (p = 0.018) groups.\nAs the sample size increases: - With n = 50, the distribution is wide and noisy — there’s considerable variation, and the true mean difference (0.004) is not clearly distinguishable from zero. - At n = 200, the distribution begins to resemble a bell curve, and the true difference is more centered. - At n = 500, the histogram tightens noticeably, and zero begins to lie closer to the tail of the distribution. - By n = 1000, the sample mean differences are tightly clustered around 0.004, and zero is clearly no longer at the center — it lies in the tail, suggesting strong evidence against the null of no effect.\nThis simulation confirms that larger samples reduce sampling variability and make it easier to detect small, true differences — exactly what Karlan and List leveraged with their large-scale experiment."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset containing information on engineering firms and their patenting activity. This includes whether or not a firm uses Blueprinty’s software.\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe first few rows of the dataset show each firm’s number of patents, regional location, age, and whether they are a Blueprinty customer. We see a mix of customer (iscustomer = 1) and non-customer firms, with patent counts ranging from 0 to 4. Firm ages vary from around 24 to 38 years, and regions span categories like Midwest and Southwest. These observations suggest the data is well-structured for count-based modeling and that customer status, region, and firm age may all influence patenting outcomes."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe begin by loading the dataset containing information on engineering firms and their patenting activity. This includes whether or not a firm uses Blueprinty’s software.\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe first few rows of the dataset show each firm’s number of patents, regional location, age, and whether they are a Blueprinty customer. We see a mix of customer (iscustomer = 1) and non-customer firms, with patent counts ranging from 0 to 4. Firm ages vary from around 24 to 38 years, and regions span categories like Midwest and Southwest. These observations suggest the data is well-structured for count-based modeling and that customer status, region, and firm age may all influence patenting outcomes."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#histogram-and-mean-comparison-by-customer-status",
    "href": "blog/project2/hw2_questions.html#histogram-and-mean-comparison-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Histogram and Mean Comparison by Customer Status",
    "text": "Histogram and Mean Comparison by Customer Status\nWe begin by comparing the distribution and average number of patents between firms that use Blueprinty’s software and those that do not.\n\n\n\n\n\n\n\n\nFigure 1: Distribution of Number of Patents by Customer Status"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#summary-statistics-of-patent-counts-by-customer-status",
    "href": "blog/project2/hw2_questions.html#summary-statistics-of-patent-counts-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Summary Statistics of Patent Counts by Customer Status",
    "text": "Summary Statistics of Patent Counts by Customer Status\n\n\n\n\nTable 1: Summary Statistics of Patents by Customer Status\n\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\ncustomer_status\n\n\n\n\n\n\n\nCustomer\n4.133056\n2.546846\n481\n\n\nNon-Customer\n3.473013\n2.225060\n1019\n\n\n\n\n\n\n\n\n\n\n\n\nKey Findings: Patent Counts by Customer Status\n\nBlueprinty customers had a higher average number of patents (4.13) compared to non-customers (3.47), based on the summary statistics.\nThe distribution of patent counts (see histogram) shows that customers are more likely to have larger patent portfolios, with a noticeable rightward skew relative to non-customers.\nThis pattern suggests a potential positive association between using Blueprinty’s software and increased patent productivity.\nHowever, this is only a descriptive comparison. Differences may be due to other firm characteristics such as region or age. Further causal modeling (e.g., regression analysis or matching) is needed to determine whether the use of Blueprinty’s software actually drives higher patent output.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#comparison-of-regions-and-ages-by-customer-status",
    "href": "blog/project2/hw2_questions.html#comparison-of-regions-and-ages-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Comparison of Regions and Ages by Customer Status",
    "text": "Comparison of Regions and Ages by Customer Status\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Distribution of Firm Age by Customer Status\n\n\n\n\n\n\n• Regional Differences: The bar chart comparing customer status by region (Figure Figure 2) shows substantial variation in software adoption across regions. For example, the Northeast has a notably higher number of Blueprinty customers compared to non-customers, while regions like the Midwest and Southwest are dominated by non-customers. This suggests that geographic location may influence adoption—potentially due to industry concentration, marketing reach, or regional innovation ecosystems.\n• Age Differences: The boxplot in Figure Figure 3 indicates that customer firms tend to be slightly older than non-customers. While the median age is only modestly higher for customers, the distribution shows a broader spread and slightly higher upper quartiles, suggesting some older firms are more likely to adopt the software.\n• Conclusion: These findings imply that both region and firm age may confound the observed relationship between customer status and patent output. Any further analysis aiming to assess the impact of Blueprinty’s software should control for these variables to avoid biased conclusions.\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nEstimation of Simple Poisson Model\nWe assume the number of patents per firm, \\(Y_i\\), follows a Poisson distribution: \\(Y_i \\sim \\text{Poisson}(\\lambda)\\). The probability mass function is:\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood for \\(n\\) observations is:\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nWe compute this value below for a given \\(\\lambda\\).\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Load data\ndf = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project2/blueprinty.csv\")\ny = df[\"patents\"]\n\ndef poisson_log_likelihood(y, lamb):\n    n = len(y)\n    return -n * lamb + y.sum() * np.log(lamb) - gammaln(y + 1).sum()\n\nlambda_val = 3.5\nloglik = poisson_log_likelihood(y, lambda_val)\nprint(f\"Poisson log-likelihood at lambda = {lambda_val}: {loglik:.4f}\")\n\nPoisson log-likelihood at lambda = 3.5: -3374.8661\n\n\n\n\nInterpretation of Poisson Log-Likelihood\nThe Poisson log-likelihood evaluated at \\(\\lambda = 3.5\\) is:\nPoisson log-likelihood at lambda = 3.5: −3374.8661\nThis value is higher (less negative) than the log-likelihood computed at \\(\\lambda = 4.0\\), which was approximately −3386.84. Therefore, a rate parameter of \\(\\lambda = 3.5\\) provides a better fit to the observed patent count data under the Poisson model.\n\nNote: This is still a trial value. To find the best-fitting \\(\\lambda\\), we must identify the value that maximizes the log-likelihood function. In the next step, we estimate the Maximum Likelihood Estimator (MLE) numerically.\n\n\n\nPoisson Log-Likelihood Curve for Different Values of \\(\\lambda\\)\n\n\n\n\n\n\n\n\nFigure 4: Poisson Log-Likelihood as a Function of λ\n\n\n\n\n\n\n\nInterpretation of Log-Likelihood Curve\nThe figure (Figure Figure 4) displays the Poisson log-likelihood as a function of the rate parameter \\(\\lambda\\), based on the observed patent counts.\n\nThe x-axis represents different values of \\(\\lambda\\), which is the assumed average number of patents per firm.\nThe y-axis shows the corresponding log-likelihood of the data under the Poisson model for each value of \\(\\lambda\\).\n\nWe observe that the log-likelihood curve reaches its maximum around \\(\\lambda \\approx 3.68\\), indicating the value that best fits the data under the Poisson assumption. This value is the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\).\nThe red dashed line on the graph marks the sample mean of the observed patent counts: [ _{} = {Y} = 3.6847 ]\nSince the log-likelihood function is concave, this plot visually confirms that the MLE is both valid and unique. The alignment between the peak of the curve and the sample mean also confirms that, under a simple Poisson model, the MLE of \\(\\lambda\\) is the sample mean.\n\n\nDeriving the MLE for \\(\\lambda\\) Analytically\nWe take the first derivative of the log-likelihood function and solve for \\(\\lambda\\):\n\\[\n\\log \\mathcal{L}(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nTaking the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\Rightarrow\n\\lambda = \\frac{\\sum Y_i}{n} = \\bar{Y}\n\\]\nNow we verify this with code:\n\nimport pandas as pd\ndf = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project2/blueprinty.csv\")\nY = df[\"patents\"]\nlambda_mle = Y.mean()\nlambda_mle\n\n3.6846666666666668\n\n\n\n\nEstimated \\(\\lambda\\) from Maximum Likelihood\nUsing numerical optimization of the Poisson log-likelihood function, we find that the maximum likelihood estimate (MLE) of \\(\\lambda\\) is:\nλ̂ = 3.6846666666666668\nThis value represents the estimated average number of patents per firm over the 5-year period. It corresponds to the value of \\(\\lambda\\) that maximizes the likelihood of the observed data under the Poisson model.\nAs expected, this estimate matches the sample mean of the observed patent counts:\n[ _{} = {Y} ]\nThis confirms both the analytical derivation and numerical optimization.\n\n\nMaximum Likelihood Estimation of \\(\\lambda\\) via Optimization\nWe now find the MLE of \\(\\lambda\\) by optimizing the log-likelihood function using scipy.optimize.minimize in Python.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\ndf = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project2/blueprinty.csv\")\nY_data = df[\"patents\"]\n\ndef poisson_log_likelihood(y, lamb):\n    n = len(y)\n    return -n * lamb + y.sum() * np.log(lamb) - gammaln(y + 1).sum()\n\ndef neg_poisson_log_likelihood(lambd):\n    return -poisson_log_likelihood(Y_data, lambd[0])\n\ninitial_guess = [Y_data.mean()]\nresult = minimize(neg_poisson_log_likelihood, x0=initial_guess, bounds=[(1e-5, None)])\n\nestimated_lambda = result.x[0]\nestimated_lambda\n\n3.6846666666666668\n\n\nThe optimization routine returns an estimated \\(\\hat{\\lambda} \\approx 3.6847\\), which confirms our earlier analytical result that the MLE of \\(\\lambda\\) is equal to the sample mean under a Poisson model.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nPoisson Regression Log-Likelihood Function\nWe now define the log-likelihood function for the Poisson regression model. This function takes in:\n\na parameter vector \\(\\beta\\),\na response vector \\(Y\\), and\na covariate matrix \\(X\\),\n\nand assumes the inverse link function \\(g^{-1}(\\cdot) = \\exp(\\cdot)\\) so that:\n\\[\n\\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis guarantees \\(\\lambda_i &gt; 0\\) for all observations, as required in the Poisson model. The resulting log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^{n} \\left( Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!) \\right)\n\\]\nBelow is the corresponding Python implementation.\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64).reshape(-1)  # Force float64 1D\n    X = np.asarray(X, dtype=np.float64)\n    Y = np.asarray(Y, dtype=np.float64)\n\n    X_beta = np.dot(X, beta)\n    lambd = np.exp(X_beta)\n\n    return np.sum(Y * np.log(lambd) - lambd - gammaln(Y + 1))\n\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv(\"/Users/hanhuazhu/Desktop/mywebsite/blog/project2/blueprinty.csv\")\n\n# Add a new column: age squared\ndf[\"age_sq\"] = df[\"age\"] ** 2\n\n# Create dummy variables for the 'region' column\nX_region = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Create the full design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_sq\", \"iscustomer\"]],\n    X_region\n], axis=1)\n\n# Convert predictors and outcome to numpy arrays\nX_np = X.to_numpy(dtype=np.float64)\nY_np = df[\"patents\"].astype(float).to_numpy()\n\n\nX_clean = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"), \n    df[[\"age\", \"age_sq\", \"iscustomer\"]].astype(float),\n    pd.get_dummies(df[\"region\"], drop_first=True).astype(float)\n], axis=1)\n\nX_np = X_clean.to_numpy(dtype=np.float64)  \nY_np = df[\"patents\"].astype(float).to_numpy()\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_poisson_regression_log_likelihood(beta, Y, X):\n    return -poisson_regression_log_likelihood(beta, Y, X)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_np)  \n\n\ninitial_beta = np.zeros(X_scaled.shape[1])  \n\nresult = minimize(\n    fun=neg_poisson_regression_log_likelihood,\n    x0=initial_beta,\n    args=(Y_np, X_scaled),\n    method='BFGS'\n)\n\nbeta_hat = result.x\nbeta_hat\n\narray([ 0.        ,  4.86327848, -5.39349539,  0.25685043,  0.04115259,\n       -0.0186009 ,  0.05259172,  0.06118601])\n\n\n\ncov_beta = result.hess_inv\n\nse_beta = np.sqrt(np.diagonal(cov_beta))\n\nregression_output = pd.DataFrame({\n    \"Feature\": X.columns,          \n    \"Coefficient\": beta_hat,\n    \"Standard Error\": se_beta\n})\n\nregression_output\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nStandard Error\n\n\n\n\n0\nintercept\n0.000000\n1.000000\n\n\n1\nage\n4.863278\n0.023264\n\n\n2\nage_sq\n-5.393495\n0.034351\n\n\n3\niscustomer\n0.256850\n0.024364\n\n\n4\nNortheast\n0.041153\n0.046839\n\n\n5\nNorthwest\n-0.018601\n0.037803\n\n\n6\nSouth\n0.052592\n0.048988\n\n\n7\nSouthwest\n0.061186\n0.049165\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(Y_np, X_np, family=sm.families.Poisson())\nglm_results = glm_model.fit()\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nWed, 28 May 2025\nDeviance:\n2143.3\n\n\nTime:\n21:13:09\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe estimated a Poisson Generalized Linear Model (GLM) using statsmodels.GLM with a log link function, appropriate for modeling count data (e.g., number of patents per firm).\n\n\nModel Specification\n\nFamily: Poisson\nLink Function: Log\nFitting Method: IRLS (Iteratively Reweighted Least Squares)\nData: 1,500 engineering firms\nPredictors:\n\nIntercept (constant)\nAge of the firm\nAge squared (to capture nonlinearity)\nBinary indicator for Blueprinty customer status\nRegion dummy variables (with one region dropped as reference)\n\n\nThe model takes the form:\nThe model takes the form:\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\cdot \\text{age}_i + \\beta_2 \\cdot \\text{age}_i^2 + \\beta_3 \\cdot \\text{iscustomer}_i + \\text{region effects}\n\\]\n\n\nModel Fit Summary\n\n\n\n\n\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nLog-Likelihood\n-3258.1\nHigher (less negative) = better fit\n\n\nDeviance\n2143.3\nLower values = better fit\n\n\nPearson Chi²\n2070.0\nLarge values may suggest overdispersion\n\n\nPseudo R² (McFadden)\n0.1360\nModel explains ~13.6% of the deviance\n\n\nNo. of Iterations\n5\nConvergence in 5 IRLS iterations\n\n\n\n\n\n\nCoefficient Interpretation\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\np-value\n95% CI\nInterpretation\n\n\n\n\nIntercept\n-0.509\n0.183\n0.005\n[-0.868, -0.150]\nBaseline log-count when all predictors = 0\n\n\nAge\n0.149\n0.014\n&lt;0.001\n[0.121, 0.176]\nPatent counts increase with age\n\n\nAge²\n-0.003\n~0.000\n&lt;0.001\n[-0.003, -0.002]\nDiminishing returns → inverted-U shape\n\n\nCustomer\n0.208\n0.031\n&lt;0.001\n[0.147, 0.268]\nBlueprinty customers have more patents\n\n\nRegions\nMixed signs\nNot significant\n0.28–0.74\nIncludes SW, NE, NW, South\nNo statistically significant regional effects\n\n\n\n\n\n\nConclusion\n\nThere is strong evidence that both firm age and being a Blueprinty customer significantly affect patent output.\nThe quadratic age term shows an inverted-U pattern: the marginal gain in patents from age diminishes.\nRegion effects are not statistically significant in this model, suggesting that once age and customer status are controlled for, regional location has little additional predictive power.\n\n\n\n\nComparison of MLE and GLM Estimates (with Standardized Inputs)\n\nimport pandas as pd\nimport numpy as np\n\nglm_params = glm_results.params\nglm_se = glm_results.bse\ncomparison_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"MLE Estimate\": beta_hat,\n    \"MLE Std. Err\": se_beta,\n    \"GLM Estimate\": glm_params,\n    \"GLM Std. Err\": glm_se\n})\n\ncomparison_df\n\n\n\n\n\n\n\n\nVariable\nMLE Estimate\nMLE Std. Err\nGLM Estimate\nGLM Std. Err\n\n\n\n\n0\nintercept\n0.000000\n1.000000\n-0.508920\n0.183179\n\n\n1\nage\n4.863278\n0.023264\n0.148619\n0.013869\n\n\n2\nage_sq\n-5.393495\n0.034351\n-0.002970\n0.000258\n\n\n3\niscustomer\n0.256850\n0.024364\n0.207591\n0.030895\n\n\n4\nNortheast\n0.041153\n0.046839\n0.029170\n0.043625\n\n\n5\nNorthwest\n-0.018601\n0.037803\n-0.017575\n0.053781\n\n\n6\nSouth\n0.052592\n0.048988\n0.056561\n0.052662\n\n\n7\nSouthwest\n0.061186\n0.049165\n0.050576\n0.047198\n\n\n\n\n\n\n\n\n\nInterpretation of MLE and GLM Estimates\nThe table above compares coefficient estimates and standard errors obtained from two estimation methods for a Poisson regression model: (1) a manually implemented Maximum Likelihood Estimation (MLE) using scipy.optimize.minimize, and (2) a Generalized Linear Model (GLM) using statsmodels.GLM.\nBoth models were fitted to the same dataset with standardized predictors, including an intercept, firm age, age squared, Blueprinty customer status, and region indicators (excluding the reference region).\n\nModel Agreement\nThe MLE and GLM approaches yield consistent results in both direction and relative magnitude of the coefficient estimates. Standard errors are also closely aligned, suggesting that both estimation procedures are implemented correctly and produce stable results under comparable scaling.\n\n\nVariable-Specific Observations\n\nIntercept: The MLE method estimates the intercept at zero, while the GLM method returns a slightly negative value (-0.509). This discrepancy is likely attributable to differences in standardization treatment, particularly in how the intercept is handled.\nAge: Both models identify age as a positive and statistically significant predictor of patent activity. The positive coefficient indicates that older firms tend to receive more patents, holding other variables constant.\nAge Squared: The negative coefficient on age squared confirms a concave (inverted-U) relationship between age and patent counts. This suggests diminishing returns to age—older firms experience reduced patent growth beyond a certain point.\nCustomer Status: The coefficient for the iscustomer variable is positive and statistically significant in both models. This provides empirical support for the claim that firms using Blueprinty’s software tend to secure more patents than non-customers, after adjusting for other firm characteristics.\nRegion Indicators: None of the regional dummy variables are statistically significant in either model. This suggests that, conditional on age and customer status, regional differences do not have a meaningful impact on patent outcomes in this dataset.\n\n\n\nConclusion\nThe high degree of alignment between the MLE and GLM results provides confidence in the robustness of the findings. The evidence strongly supports the role of firm age and Blueprinty software usage in explaining variation in patent counts, while region appears to play a minor role.\n\n\n\nEstimating the Effect of Blueprinty Software on Patent Success\nTo interpret the model in terms of predicted patent counts (rather than log-count coefficients), we simulate two counterfactual scenarios:\n\nX_0: Every firm is treated as a non-customer (iscustomer = 0)\nX_1: Every firm is treated as a customer (iscustomer = 1)\n\nUsing the fitted model coefficients, we compare the predicted number of patents under both cases and compute the average difference.\n\nX_no_customer = X_clean.copy()\nX_no_customer.loc[:, \"iscustomer\"] = 0\n\nX_as_customer = X_clean.copy()\nX_as_customer.loc[:, \"iscustomer\"] = 1\nX_no_cust_np = X_no_customer.to_numpy()\nX_yes_cust_np = X_as_customer.to_numpy()\npatents_pred_no = glm_results.predict(X_no_cust_np)\npatents_pred_yes = glm_results.predict(X_yes_cust_np)\n\neffect_of_customer = np.mean(patents_pred_yes - patents_pred_no)\neffect_of_customer\n\n0.792768071045253\n\n\n\n\nEstimated Effect of Blueprinty’s Software on Patent Success\nTo interpret the practical impact of Blueprinty’s software, we simulated two counterfactual scenarios:\n\nIn the first, we treated all firms as non-customers (iscustomer = 0).\nIn the second, we treated all firms as customers (iscustomer = 1).\n\nWe used our fitted Poisson regression model to predict the number of patents under each scenario. The difference in predicted patent counts for each firm reflects the estimated effect of Blueprinty’s software. We then took the average of these differences across all firms in the dataset.\nThe result:\nAverage treatment effect ≈ 0.793 patents\n\nInterpretation:\nOn average, firms that use Blueprinty’s software are predicted to obtain approximately 0.79 more patents over the observed period compared to firms that do not use the software, holding all other factors constant (including age and region).\nThis suggests a meaningful positive association between Blueprinty software usage and patent output. While this estimate does not imply causality, it provides strong evidence that customers of Blueprinty tend to outperform non-customers in terms of patent success when matched on observable characteristics."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#modeling-airbnb-booking-activity",
    "href": "blog/project2/hw2_questions.html#modeling-airbnb-booking-activity",
    "title": "Poisson Regression Examples",
    "section": "Modeling Airbnb Booking Activity",
    "text": "Modeling Airbnb Booking Activity\nIn this section, we assume that the number of reviews is a valid proxy for the number of bookings. We conduct exploratory data analysis, clean the dataset, and estimate a Poisson regression model to explain variation in review counts based on listing characteristics.\n\ncheck the dataset\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nSummary Statistics of Key Variables\nBefore modeling, we examine basic descriptive statistics to understand the scale and distribution of the data.\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ndays\n40628.0\n1102.37\n1383.27\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\nbathrooms\n40468.0\n1.12\n0.39\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.15\n0.69\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n40628.0\n144.76\n210.66\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nnumber_of_reviews\n40628.0\n15.90\n29.25\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\nreview_scores_cleanliness\n30433.0\n9.20\n1.12\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.41\n0.84\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.33\n0.90\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nunique\ntop\nfreq\n\n\n\n\nroom_type\n40628\n3\nEntire home/apt\n19873\n\n\ninstant_bookable\n40628\n2\nf\n32759\n\n\n\n\n\n\n\n\n\nSummary Statistics for Numeric Variables\nWe present summary statistics for selected numeric variables that will be used in our modeling. This includes central tendency, dispersion, and count of non-missing observations.\n\n\n\n\n\n\n\n\n\nN\nMean\nStd Dev\nMin\nQ1\nMedian\nQ3\nMax\n\n\n\n\ndays\n40628.0\n1102.37\n1383.27\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\nbathrooms\n40468.0\n1.12\n0.39\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.15\n0.69\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n40628.0\n144.76\n210.66\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nnumber_of_reviews\n40628.0\n15.90\n29.25\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\nreview_scores_cleanliness\n30433.0\n9.20\n1.12\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.41\n0.84\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.33\n0.90\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Number of Reviews\n\n\n\n\nThis histogram shows a highly right-skewed distribution of the number of reviews per Airbnb listing. Most listings receive fewer than 20 reviews, with a sharp drop-off beyond that point. A small number of listings have 50 or more reviews, indicating that a few highly booked properties dominate review volume. This skewness supports the use of a Poisson (or possibly negative binomial) model for count-based regression.\n\n\nDistribution of Number of Reviews by Room Type\nThe plot below shows the full distribution of review counts (a proxy for bookings) across different room types. Violin plots provide more detail than boxplots by visualizing the density of the data.\n\n\n\n\n\nDistribution of Number of Reviews by Room Type\n\n\n\n\nThe violin plot above illustrates the distribution of the number of reviews (used here as a proxy for bookings) across different room types: Private Room, Entire Home/Apt, and Shared Room. Each distribution is highly right-skewed, with the majority of listings receiving relatively few reviews and a long tail of listings with many.\nWhile all room types follow a similar overall pattern, there are some subtle differences in the spread and density. Private rooms and entire home/apartment listings show comparable distributions, both with slightly higher concentrations of listings receiving moderate review counts. Shared rooms, on the other hand, exhibit a more compressed distribution, suggesting they may receive fewer bookings overall, or are listed less frequently.\nOverall, the visual supports including room_type as a categorical predictor in a Poisson regression model, though the similarities across categories suggest that its impact on bookings may be modest compared to other factors like price or location.\n## Exploratory Visualizations\nWe include several plots to explore how various listing characteristics relate to the number of reviews, which serves as a proxy for bookings.\n\n\n\nReviews vs. Price\nThis scatter plot shows how review counts vary with listing price. Since price is highly skewed, we limit the x-axis to exclude extreme outliers.\n\n\n\n\n\nReview Counts vs. Listing Price\n\n\n\n\n\n\n\nReviews vs. Days Listed\nThis plot explores whether longer-listed units tend to accumulate more reviews over time.\n\n\n\n\n\nReview Counts vs. Days Listed\n\n\n\n\n\n\n\nReviews by Instant Bookable Status\nBoxplot comparing listings that are instantly bookable versus not. This can reveal whether frictionless booking impacts demand.\n\n\n\n\n\nNumber of Reviews by Instant Bookable Status\n\n\n\n\n\n\n\nReviews by Cleanliness Score\nThis boxplot evaluates whether listings with higher cleanliness ratings tend to get more reviews.\n\n\n\n\n\nNumber of Reviews by Cleanliness Score\n\n\n\n\n\n\n\nListings per Room Type\nThis count plot shows the volume of listings per room type. It provides useful context on sample size for each category.\n\n\n\n\n\nNumber of Listings by Room Type\n\n\n\n\n\n\nExploratory Visualizations: Key Insights\n\nReviews vs. Price\nThe first scatter plot shows that review counts are highest for listings priced between $50 and $150. While there are some listings with high prices and high review counts, the general trend suggests diminishing review volume at higher price points. This indicates that extremely expensive listings may be less frequently booked, or cater to niche audiences.\n\n\nReviews vs. Days Listed\nListings that have been active on the platform for a longer time tend to accumulate more reviews, as shown in the second plot. The positive trend aligns with expectations: more time on the platform allows for more bookings and reviews to accumulate. However, the scatter remains wide, suggesting that duration alone does not fully explain booking activity.\n\n\nReviews by Instant Bookable Status\nThe third plot compares review counts between listings that are instantly bookable and those that are not. While both groups exhibit a similar spread, instantly bookable listings show slightly higher medians and more frequent high-review outliers. This implies that reducing booking friction may enhance listing performance.\n\n\nReviews by Cleanliness Rating\nReview counts appear to increase with higher cleanliness scores, particularly between ratings 7 and 10. Listings rated 9 or 10 in cleanliness show a higher density of large review counts, suggesting that cleanliness positively correlates with booking success and guest satisfaction.\n\n\nListing Counts by Room Type\nThe bar chart shows that “Entire home/apt” and “Private room” are by far the most common listing types, while “Shared room” is relatively rare. This distribution reflects user preferences for privacy and autonomy in accommodations, and justifies including room type as a categorical predictor in modeling.\n\n\n\nPoisson Regression Model: Predicting Review Counts from Listing Features\nWe use a Poisson regression model to predict the number of reviews (a proxy for bookings) based on listing characteristics, amenities, and quality indicators.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Load and clean data\ndf = pd.read_csv(\"~/Downloads/airbnb.csv\")\n\n# Select relevant variables and drop missing rows\nselected_vars = [\n    \"days\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\", \"room_type\"\n]\ndf_model = df[selected_vars + [\"number_of_reviews\"]].dropna()\n\n# Binary encode instant_bookable\ndf_model[\"instant_bookable\"] = (df_model[\"instant_bookable\"] == \"t\").astype(int)\n\n# One-hot encode room_type (drop reference)\nroom_dummies = pd.get_dummies(df_model[\"room_type\"], drop_first=True)\n\n# Construct design matrix\nX_model = pd.concat([\n    pd.Series(1, index=df_model.index, name=\"intercept\"),\n    df_model[[\n        \"days\", \"bathrooms\", \"bedrooms\", \"price\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Response variable\nY_model = df_model[\"number_of_reviews\"]\n\n# Ensure both X and Y are float64 and index-aligned\nX_model = X_model.astype(float)\nY_model = Y_model.astype(float)\n\n# Fit GLM Poisson model\nglm_model = sm.GLM(Y_model, X_model, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2418e+05\n\n\nDate:\nWed, 28 May 2025\nDeviance:\n9.2689e+05\n\n\nTime:\n21:13:10\nPearson chi2:\n1.37e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.6840\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nintercept\n3.4980\n0.016\n217.396\n0.000\n3.467\n3.530\n\n\ndays\n5.072e-05\n3.91e-07\n129.755\n0.000\n5e-05\n5.15e-05\n\n\nbathrooms\n-0.1177\n0.004\n-31.394\n0.000\n-0.125\n-0.110\n\n\nbedrooms\n0.0741\n0.002\n37.197\n0.000\n0.070\n0.078\n\n\nprice\n-1.791e-05\n8.33e-06\n-2.151\n0.031\n-3.42e-05\n-1.59e-06\n\n\nreview_scores_cleanliness\n0.1131\n0.001\n75.611\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0769\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0911\n0.002\n-50.490\n0.000\n-0.095\n-0.088\n\n\ninstant_bookable\n0.3459\n0.003\n119.666\n0.000\n0.340\n0.352\n\n\nPrivate room\n-0.0105\n0.003\n-3.847\n0.000\n-0.016\n-0.005\n\n\nShared room\n-0.2463\n0.009\n-28.578\n0.000\n-0.263\n-0.229\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nThe table above summarizes the results of a Poisson regression model predicting the number of reviews a listing receives based on various listing attributes.\n\nModel Fit\n\nLog-Likelihood: –524,180\n\nPseudo R²: 0.6840\n\nNo. of Observations: 30,160\n\nThe model explains a substantial amount of variation in review counts, as indicated by the high Pseudo R² value (0.684), which is unusually strong for a Poisson model.\n\n\n\n\nKey Coefficients and Interpretation\nAll coefficients are interpreted on the log-count scale due to the log-link function. A positive coefficient means that the predictor increases the expected number of reviews, holding other variables constant.\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\nIntercept\n3.4980\nBaseline expected log-count of reviews when all predictors are zero.\n\n\nDays Listed\n5.07e-05\nListings on the platform longer tend to have more reviews. Small but significant.\n\n\nBathrooms\n–0.1177\nEach additional bathroom is associated with a decrease in expected reviews, possibly reflecting listing type or overpricing.\n\n\nBedrooms\n0.0741\nEach additional bedroom increases expected review counts, likely due to larger or more desirable units.\n\n\nPrice\n–1.79e-05\nSmall negative effect; higher-priced listings may receive fewer bookings (and reviews).\n\n\nReview Score: Cleanliness\n0.1131\nHigher cleanliness ratings are associated with more reviews, suggesting guest satisfaction leads to more repeat use and visibility.\n\n\nReview Score: Location\n–0.0769\nSurprisingly negative; may indicate that extremely central or remote listings attract niche usage patterns.\n\n\nReview Score: Value\n–0.0911\nNegative effect may reflect that “value” is inversely related to pricing in perception.\n\n\nInstant Bookable\n0.3459\nListings that are instantly bookable get significantly more reviews, indicating reduced friction increases booking volume.\n\n\n\n\n\n\nConclusion\nThe model reveals that factors such as number of bedrooms, cleanliness, and instant bookable status are strong positive predictors of booking volume (as proxied by review count). In contrast, higher prices, more bathrooms, and lower perceived value are associated with fewer bookings. These insights are helpful for hosts optimizing listing strategies and Airbnb improving search algorithms.\n\n\n\nFinal Reflections\nThis project showcased how Poisson regression can be applied to understand patterns in real-world count data. Whether analyzing how often firms receive patents or how frequently Airbnb listings accumulate reviews, the objective was to identify which features meaningfully influence these outcomes.\nThe Blueprinty analysis emphasized how organizational characteristics—like age and region—affect innovation outcomes, and showed a measurable advantage tied to using the firm’s software. In contrast, the Airbnb study focused on listing attributes such as pricing, room setup, cleanliness ratings, and booking convenience. Across both cases, Poisson modeling proved valuable in capturing the relationship between structured inputs and count-based performance measures.\nConstructing the likelihood function manually, then benchmarking it against a built-in GLM, offered more than just technical comparison—it reinforced the benefits of building from the ground up. While the outputs were often aligned, the hands-on process highlighted important details around implementation and diagnostics that wouldn’t be as apparent using pre-built packages alone.\nIn sum, this work illustrates how statistically grounded, interpretable models can offer clear insights for data-driven decision-making across a variety of practical settings."
  }
]