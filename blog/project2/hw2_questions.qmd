---
title: "Poisson Regression Examples"
author: "Hanhua Zhu"
date: May 7, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data
We begin by loading the dataset containing information on engineering firms and their patenting activity. This includes whether or not a firm uses Blueprinty's software.

```{python}
#| echo: false

import pandas as pd
df = pd.read_csv("/Users/hanhuazhu/Desktop/mywebsite/blog/project2/blueprinty.csv")
df.head()
```
The first few rows of the dataset show each firm's number of patents, regional location, age, and whether they are a Blueprinty customer. We see a mix of customer (`iscustomer = 1`) and non-customer firms, with patent counts ranging from 0 to 4. Firm ages vary from around 24 to 38 years, and regions span categories like Midwest and Southwest. These observations suggest the data is well-structured for count-based modeling and that customer status, region, and firm age may all influence patenting outcomes.


_todo: Compare histograms and means of number of patents by customer status. What do you observe?_
## Patent Counts by Customer Status

To understand whether firms using Blueprinty's software tend to receive more patents, we examine the distribution of patent counts by customer status. The plot below shows histograms for customers and non-customers side by side, and the table reports the average number of patents for each group.

```{python}
#| echo: false

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

plt.figure(figsize=(8, 5))
sns.histplot(data=df, x="patents", hue="iscustomer", bins=15, multiple="dodge", palette="muted", shrink=0.8)
plt.title("Distribution of Patents by Customer Status")
plt.xlabel("Number of Patents")
plt.ylabel("Count of Firms")
plt.legend(title="Is Customer (1 = Yes)")
plt.tight_layout()
plt.show()
```
The histogram above shows the distribution of patent counts for firms that do and do not use Blueprinty’s software. While the majority of both groups have relatively few patents, we observe that Blueprinty customers are more likely to appear in higher patent count categories (e.g., 5 or more patents). In contrast, non-customers dominate the lower count bins (0–3 patents). This suggests that firms using Blueprinty’s software may be associated with higher patenting activity, though further analysis is needed to account for other factors like firm age and region.

```{python}
#| echo: false

# Calculate and display group means
df.groupby("iscustomer")["patents"].mean().reset_index(name="Average Number of Patents")
```
On average, Blueprinty customers have approximately 0.66 more patents than non-customers. While this difference is modest, it suggests a positive association between using the software and patenting activity. This pattern aligns with what we observed in the histogram and sets the stage for more rigorous modeling using Poisson regression.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

## Differences in Firm Characteristics by Customer Status

Blueprinty customers are not selected at random. It may be important to account for systematic differences in characteristics like region and firm age between customers and non-customers.

We begin by examining whether the age distribution differs by customer status, and then compare regional distributions.

```{python}
#| echo: false

import seaborn as sns
import matplotlib.pyplot as plt

# Age distribution
plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x="iscustomer", y="age", palette="muted")
plt.title("Firm Age by Customer Status")
plt.xlabel("Is Customer (1 = Yes)")
plt.ylabel("Firm Age")
plt.tight_layout()
plt.show()
```
The boxplot reveals that Blueprinty customers tend to be slightly younger than non-customer firms. While both groups share similar age ranges and interquartile spreads, the median firm age is lower among customers. This suggests that firm age may influence both software adoption and patenting behavior. As a result, it is important to control for age in subsequent modeling to ensure we isolate the effect of using Blueprinty's software from differences attributable to firm maturity.




The table shows the proportion of Blueprinty customers and non-customers across different regions. Most regions — including the Midwest, Northwest, South, and Southwest — have a relatively low customer adoption rate (about 16–18%). However, the Northeast stands out: a majority of firms in this region (55%) are Blueprinty customers.
```{python}
#| echo: false

# Region distribution by customer status
pd.crosstab(df["region"], df["iscustomer"], normalize='index').round(2)
```
This regional disparity suggests that firms in the Northeast are significantly more likely to use Blueprinty's software. As a result, region may confound the relationship between software usage and patent success, and it should be accounted for in any regression model to avoid biased estimates.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

_todo: Write down mathematically the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.
### Poisson Likelihood Function

Let \( Y_i \sim \text{Poisson}(\lambda_i) \), where \( \lambda_i \) is the expected number of patents for firm \( i \).

The probability mass function for a Poisson-distributed outcome is:

\[
f(Y_i | \lambda_i) = \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
\]

Assuming observations are independent across \( n \) firms, the **likelihood function** is:

\[
\mathcal{L}(\boldsymbol{\lambda}) = \prod_{i=1}^n \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
\]

Taking the log gives the **log-likelihood**:

\[
\ell(\boldsymbol{\lambda}) = \sum_{i=1}^n \left( -\lambda_i + Y_i \log \lambda_i - \log Y_i! \right)
\]

In Poisson regression, we model \( \lambda_i = \exp(X_i \beta) \), which ensures \( \lambda_i > 0 \).

### Coding the Log-Likelihood Function

We define the log-likelihood function for a Poisson model, where the outcome variable \( Y \) follows a Poisson distribution with mean \( \lambda \). This function will be useful for implementing MLE manually.

```{python}
#| echo: true

import numpy as np

def poisson_loglikelihood(lam, y):
    """
    Compute the log-likelihood for Poisson-distributed outcomes.
    
    Parameters:
    - lam: array-like of expected values (lambda_i)
    - y: array-like of observed counts (Y_i)
    
    Returns:
    - total log-likelihood (float)
    """
    lam = np.asarray(lam)
    y = np.asarray(y)
    
    loglik = np.sum(-lam + y * np.log(lam) - np.log(np.math.factorial(y)))
    return loglik
```


_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._

### Log-Likelihood over a Range of λ Values

We use our Poisson log-likelihood function to evaluate how the likelihood changes over different values of λ. This helps us visualize the value of λ that best fits the observed number of patents.

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gammaln

# Use observed patent counts
y_observed = df["patents"].values

# Define the vectorized log-likelihood function
def poisson_loglikelihood(lam, y):
    lam = np.asarray(lam)
    y = np.asarray(y)
    return np.sum(-lam + y * np.log(lam) - gammaln(y + 1))

# Generate a range of lambda values to evaluate
lambda_vals = np.linspace(0.1, 10, 200)
loglik_vals = [poisson_loglikelihood(lam, y_observed) for lam in lambda_vals]

# Plot log-likelihood
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, loglik_vals, color="steelblue")
plt.title("Poisson Log-Likelihood over λ")
plt.xlabel("λ (mean patents per firm)")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.tight_layout()
plt.show()
```
The plot above shows how the Poisson log-likelihood varies with different values of λ, which represents the expected number of patents per firm. The curve peaks around λ = 4, indicating that this value maximizes the likelihood of observing the data under a Poisson model. In other words, the value λ ≈ 4 is the maximum likelihood estimate (MLE) — it best fits the distribution of observed patent counts. Values of λ much smaller or larger result in significantly lower likelihoods, confirming that λ ≈ 4 provides the most plausible explanation for the data.


_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which "feels right" because the mean of a Poisson distribution is lambda._
### Deriving the MLE for λ in the Poisson Model

Let \( Y_1, Y_2, \ldots, Y_n \) be independent observations from a Poisson distribution with parameter \( \lambda \). The log-likelihood function is:

\[
\ell(\lambda) = \sum_{i=1}^n \left( -\lambda + Y_i \log \lambda - \log Y_i! \right)
\]

To find the maximum likelihood estimate (MLE), take the derivative with respect to \( \lambda \), set it to zero, and solve:

\[
\frac{d\ell}{d\lambda} = \sum_{i=1}^n \left( -1 + \frac{Y_i}{\lambda} \right) = 0
\]

\[
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
\]

\[
\frac{1}{\lambda} \sum_{i=1}^n Y_i = n \quad \Rightarrow \quad \lambda = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
\]

So, the MLE of \( \lambda \) is simply the **sample mean** of the observed data:

\[
\hat{\lambda}_{\text{MLE}} = \bar{Y}
\]

This result makes intuitive sense because the Poisson distribution has a single parameter, \( \lambda \), which represents both its **mean and variance**.

_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._

### Finding the MLE of λ Using Optimization

To confirm our visual result and mathematical derivation, we numerically estimate the MLE of \( \lambda \) by maximizing the Poisson log-likelihood. Since most optimizers minimize functions, we minimize the **negative log-likelihood** using `scipy.optimize`.

```{python}
#| echo: true

from scipy.optimize import minimize_scalar

# Define the negative log-likelihood function
def neg_loglik(lambda_val):
    return -poisson_loglikelihood(lambda_val, y_observed)

# Optimize using bounded scalar minimization
result = minimize_scalar(neg_loglik, bounds=(0.1, 10), method='bounded')

# Extract the MLE
lambda_mle = result.x
lambda_mle
```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

### Defining the Poisson Regression Log-Likelihood

In the Poisson regression model, each firm \( i \) has a rate \( \lambda_i = \exp(X_i^\top \beta) \), where \( X_i \) includes the firm's age, age squared, region dummies, and customer status. We now define the log-likelihood function in terms of \( \beta \) and \( X \).

```{python}
#| echo: true

import numpy as np
from scipy.special import gammaln

def poisson_regression_loglikelihood(beta, y, X):
    """
    Compute the Poisson regression log-likelihood.

    Parameters:
    - beta: array-like, shape (k,)
    - y: array-like, shape (n,)
    - X: array-like, shape (n, k)

    Returns:
    - log-likelihood (float)
    """
    beta = np.asarray(beta)
    y = np.asarray(y)
    X = np.asarray(X)

    lambda_i = np.exp(X @ beta)  # inverse link function
    loglik = np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))
    return loglik
```


### Estimating a Poisson Regression Model with `statsmodels`

We now estimate the Poisson regression using `statsmodels`, which provides built-in handling of the link function, robust inference, and well-formatted output.

```{python}
#| echo: true

import statsmodels.api as sm
import pandas as pd

# Create age^2 and region dummies
df["age_squared"] = df["age"] ** 2
region_dummies = pd.get_dummies(df["region"], drop_first=True)

# Combine predictors
X = pd.concat([
    df[["age", "age_squared", "iscustomer"]],
    region_dummies
], axis=1)

X = sm.add_constant(X)  # add intercept

# Convert both X and y to float arrays (guaranteed numeric)
X = X.astype(float)
y = df["patents"].astype(float)

# Fit Poisson regression
poisson_model = sm.GLM(y, X, family=sm.families.Poisson())
poisson_results = poisson_model.fit()

# Show model summary
poisson_results.summary()
```


### Verifying Results Using `statsmodels.GLM`

To verify our custom likelihood and optimization results, we fit the same Poisson regression model using the built-in `statsmodels.GLM()` function with the Poisson family. This method handles link functions and inference automatically, providing coefficient estimates and standard errors.

```{python}
#| echo: true

import statsmodels.api as sm
import pandas as pd

# Create age squared and dummy variables
df["age_squared"] = df["age"] ** 2
region_dummies = pd.get_dummies(df["region"], drop_first=True)

# Build the design matrix
X = pd.concat([
    df[["age", "age_squared", "iscustomer"]],
    region_dummies
], axis=1)

X = sm.add_constant(X)  # Add intercept

# Convert to float (ensures all numeric types)
X = X.astype(float)
y = df["patents"].astype(float)

# Fit Poisson model
poisson_model = sm.GLM(y, X, family=sm.families.Poisson())
poisson_results = poisson_model.fit()

# Display the summary
poisson_results.summary()
```

he regression results provide strong evidence that using Blueprinty’s software is associated with an increase in the expected number of patents. The coefficient on iscustomer is 0.2076 and is statistically significant (p < 0.001), implying that being a customer increases the log expected patent count. Exponentiating this coefficient (i.e., 
𝑒
0.2076
≈
1.23
e 
0.2076
 ≈1.23) suggests that, all else equal, Blueprinty customers have about 23% more patents than non-customers.

Firm age is also significant: the positive coefficient on age and negative coefficient on age_squared indicate a concave (inverted-U) relationship between age and patent output — suggesting patent productivity increases with age, but at a decreasing rate.

Regional variables are not statistically significant, indicating no clear evidence that location alone explains differences in patenting after controlling for other factors.

Overall, the model supports Blueprinty’s claim that their customers are more successful in patenting, even when accounting for differences in firm age and location.

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._

### Simulated Impact of Blueprinty’s Software

We now simulate the impact of Blueprinty by predicting patent counts under two scenarios: (1) no firms use the software, and (2) all firms use the software. We then compute the average difference in predicted patents across all firms.

```{python}
#| echo: false

# Create counterfactual datasets
X_0 = X.copy()
X_1 = X.copy()
X_0["iscustomer"] = 0
X_1["iscustomer"] = 1

# Predict expected patent counts
y_pred_0 = poisson_results.predict(X_0)
y_pred_1 = poisson_results.predict(X_1)

# Calculate average treatment effect
average_effect = (y_pred_1 - y_pred_0).mean()
average_effect
```

To assess the practical impact of Blueprinty's software on patenting outcomes, we simulated two scenarios using our fitted Poisson regression model. In the first scenario, we predicted the number of patents for each firm assuming none used Blueprinty. In the second, we predicted patent counts assuming all firms were customers. The average difference between these two sets of predictions was approximately 0.79 patents per firm, indicating that Blueprinty’s software is associated with a meaningful increase in patenting success. This approach translates the regression results into an interpretable quantity and confirms that, on average, firms that use Blueprinty are expected to secure nearly one additional patent over five years compared to similar non-customers.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._


## Modeling Airbnb Reviews

We assume that the number of reviews is a good proxy for the number of bookings a listing receives. In this section, we conduct exploratory data analysis, prepare the dataset, fit a Poisson regression model, and interpret the model results.

### 1. Data Cleaning and Exploration

We begin by selecting relevant variables for predicting the number of reviews. We drop any rows with missing values to ensure clean model fitting.
```{python}
#| echo: true

import pandas as pd

# Load the Airbnb dataset (adjust the path as needed)
df = pd.read_csv("/Users/hanhuazhu/Desktop/mywebsite/blog/project2/airbnb.csv")

# Preview column names to confirm structure
df.columns
```


```{python}
#| echo: true

# Select relevant variables
airbnb = df[[
    "number_of_reviews", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", 
    "review_scores_value", "instant_bookable"
]]

# Drop missing values
airbnb = airbnb.dropna()

# Display a summary of the cleaned data
airbnb.describe(include="all")
```

### 2. Poisson Regression Model

We use Poisson regression to model the number of reviews as a function of listing features, assuming that reviews follow a Poisson distribution. We include dummy variables for categorical features and a constant term.

```{python}
#| echo: true

import statsmodels.api as sm
import pandas as pd

# Create dummy variables
dummies = pd.get_dummies(airbnb[["room_type", "instant_bookable"]], drop_first=True)

# Build design matrix
X_airbnb = pd.concat([
    airbnb[["bathrooms", "bedrooms", "price", 
            "review_scores_cleanliness", 
            "review_scores_location", 
            "review_scores_value"]],
    dummies
], axis=1)

X_airbnb = sm.add_constant(X_airbnb)

# Ensure all data are numeric
X_airbnb = X_airbnb.astype(float)
y_airbnb = airbnb["number_of_reviews"].astype(float)

# Fit Poisson regression
model_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson()).fit()

# Display model summary
model_airbnb.summary()
```
l_airbnb.summary()
```

### 3. Interpretation of Results

The coefficients from the Poisson regression represent the log change in expected number of reviews per one-unit increase in each predictor. Exponentiating these values gives us approximate percentage changes. For example, the coefficient on instant_bookable_t is 0.3344, which implies that listings that are instantly bookable receive approximately 40% more reviews than those that are not (
𝑒
0.3344
≈
1.40
e 
0.3344
 ≈1.40).

Several listing characteristics are significantly associated with review counts. Listings with more bedrooms are associated with more reviews, while more bathrooms are unexpectedly associated with fewer. Cleanliness scores are strongly and positively related to review volume, while location and value scores are negatively associated — possibly reflecting competitive pressures in highly rated areas.

Room type also plays a significant role. Compared to entire homes/apartments, private rooms receive slightly fewer reviews, while shared rooms see a substantially lower expected review count. Price has a small negative coefficient and is only marginally significant.

Overall, these results suggest that listing type, cleanliness, instant booking, and core features like bedrooms meaningfully affect how often a listing is booked (as proxied by reviews).




