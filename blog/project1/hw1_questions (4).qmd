
---
title: "A Replication of Karlan and List (2007)"
author: "Hanhua Zhu"
date: Tuesday, April 22, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Introduction
Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.

The experiment tested whether the presence of a matching donation offer (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the likelihood of giving and the amount donated. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the maximum match amount (e.g., $25,000, $50,000, or $100,000) and the suggested donation amount, which was calculated based on each recipient’s highest prior donation.

Recipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.

The results were published in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

This project seeks to replicate some of their core findings using the original dataset and statistical methods.


## Data

### Description

We load and explore the dataset from Karlan and List (2007).
```{python}
#| echo: false
#| warning: false
#| message: false

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy import stats

data = pd.read_stata("/Users/hanhuazhu/Downloads/karlan_list_2007 (2).dta")
```

## Description

The dataset includes 50,083 observations, each representing a past donor who received a fundraising letter as part of a large-scale randomized field experiment. The dataset captures a wide range of information, including treatment assignments, donation outcomes, prior giving behavior, and demographic context.

There are 51 variables in total, which can be broadly grouped into the following categories:
- **Treatment indicators**: such as `treatment`, `ratio2`, `ratio3`, `size25`, `size50`, `size100`, and `askd1`, which reflect match ratios, thresholds, and suggested donation amounts.
- **Outcome variables**: including `gave` (binary indicator for donation) and `amount` (dollar amount donated).
- **Historical giving behavior**: such as `hpa` (highest previous amount), `ltmedmra` (indicator for low prior donors), and `years` since the first donation.
- **Demographic and contextual features**: such as `female`, `couple`, `median_hhincome`, `page18_39`, `pwhite`, and `pop_propurban`.

Most variables are either complete or have minimal missingness, and the data types are a mix of binary indicators, integers, floats, and categorical values. This structure makes the dataset well-suited for regression, simulation, and visualization tasks.

Summary statistics show that approximately **66.7% of observations are in the treatment group**, with **33.3% in the control group**, as expected from random assignment. The `gave` variable indicates that about **2.06% of individuals donated**, and the average donation (`amount`) across the full sample is **$0.92**, with some contributions reaching as high as **$400**. This reflects the typical skew in donation behavior: most individuals gave nothing, while a few gave large amounts.

The match ratio flags (`ratio2`, `ratio3`) suggest roughly **equal allocation** across the 1:1, 2:1, and 3:1 groups. Likewise, the match threshold indicators (`size25`, `size50`, `size100`, `sizeno`) are each present in about **16.7%** of the sample. Suggested donation framing variables (`askd1`, `askd2`, `askd3`) also appear to be evenly distributed, reinforcing that the experimental design was properly randomized.

The table below provides descriptions for key variables in the dataset.


#:::: {.callout-note collapse="true"}


##| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

```{python}
#| echo: false
#| message: false
#| warning: false

# Overview of data structure and sample records

data.describe().T

```

```{python}
#| echo: false
#| message: false
#| warning: false

# Show summary statistics for key variables
data.describe().round(6)
```

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

To assess whether the treatment was randomly assigned, I examine five pre-treatment covariates unrelated to the outcome: `years` (years since first donation), `female` (gender indicator), `couple` (marital status), `median_hhincome` (median household income by zip code), and `page18_39` (proportion aged 18–39 in the zip code). These are useful for balance testing because they should not be affected by treatment assignment and reflect characteristics likely observed before the intervention.

For each variable, I conduct:

1. A **manual t-test**, calculated using the following formula:

$$
t = \frac{\bar{X}_{\text{treatment}} - \bar{X}_{\text{control}}}{\sqrt{\frac{s^2_{\text{treatment}}}{n_{\text{treatment}}} + \frac{s^2_{\text{control}}}{n_{\text{control}}}}}
$$

2. A **linear regression**, where the covariate is regressed on the treatment indicator:

$$
\text{covariate}_i = \alpha + \beta \cdot \text{treatment}_i + \varepsilon_i
$$

The coefficient $\beta$ captures the estimated difference in means between groups.

---

### Manual T-Test Results

- `years`: The t-statistic was **-1.091**, indicating no significant difference.
- `female`: The t-statistic was **-1.754**, just below conventional significance.
- `couple`: The t-statistic was **-0.582**, suggesting no group difference in relationship status.
- `median_hhincome`: The t-statistic was **-0.743**, showing no income-based imbalance.
- `page18_39`: The t-statistic was **-0.124**, indicating age composition is well balanced.

---

### Linear Regression Results

Each covariate was also regressed on the treatment variable:

- `years`: Coefficient = **-0.058**, p = **0.270**
- `female`: Coefficient = **-0.008**, p = **0.079**
- `couple`: Coefficient = **-0.002**, p = **0.559**
- `median_hhincome`: Coefficient = **-157.925**, p = **0.458**
- `page18_39`: Coefficient = **~0.000**, p = **0.901**

---

### Interpretation

Across all covariates, both the t-tests and linear regressions provide consistent evidence of **no statistically significant differences** between the treatment and control groups. The estimated coefficients are small, and all p-values exceed 0.05, supporting the assumption of balance under random assignment.

These results replicate the purpose of **Table 1 in Karlan and List (2007)**, which provides confidence that the estimated treatment effects are not confounded by observable pre-treatment characteristics.


## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

### Charitable Contribution Made

We examine whether the treatment group had a higher donation rate compared to the control group.
```{python}
#| echo: false
#| warning: false
#| message: false

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Create a copy to avoid altering the original DataFrame
plot_data = data.copy()
plot_data["treatment_label"] = plot_data["treatment"].map({0: "Control", 1: "Treatment"})

# Set custom colors
custom_colors = ["steelblue", "darkorange"]

sns.barplot(data=plot_data, x="treatment_label", y="gave", errorbar=None, palette=custom_colors)
plt.ylabel("Proportion Donated")
plt.title("Donation Rate by Treatment Group")
plt.show()
```
The bar chart above compares the average donation rates between individuals in the control group and those in the treatment group. The control group received a standard fundraising letter, while the treatment group received a letter that included a matching donation offer.

Although the overall donation rates are low in both groups, the treatment group shows a visibly higher proportion of individuals who donated. This difference, though modest, is consistent with the hypothesis that matching gifts encourage charitable giving. The visual evidence aligns with the statistical results from t-tests and regressions presented later in the analysis, and replicates the main findings of Karlan and List (2007), where even small psychological nudges—like a matching offer—can lead to meaningful behavioral changes.


#### T-Test for Donation Rates

This code performs an independent samples t-test to determine whether the mean donation rate (`gave`) differs significantly between the treatment and control groups. Since `gave` is a binary outcome indicating whether a donation was made, the test compares the proportion of donors across the two groups to assess the effect of receiving a matching donation offer.


```{python}
# T-test on binary outcome (gave)
treat = data[data['treatment'] == 1]['gave']
control = data[data['treatment'] == 0]['gave']

t_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)
print(f"T-test for 'gave':\nT-statistic: {t_stat:.3f}, p-value: {p_val:.3f}")
```
The result of the t-test yields a t-statistic of 3.209 with a p-value of 0.001. This indicates a statistically significant difference in donation rates between the treatment and control groups at the 1% significance level. In other words, individuals who received a matching donation offer were significantly more likely to donate than those who did not, supporting the hypothesis that such offers increase giving behavior.



To complement the t-test, we estimate a simple ordinary least squares (OLS) regression to evaluate the effect of the treatment assignment on the probability of giving. The binary outcome variable `gave` is regressed on the treatment indicator, which equals 1 for those who received a matching donation offer and 0 for the control group.

This model can be interpreted as a comparison of group means, where the intercept represents the donation rate in the control group, and the treatment coefficient captures the average difference in donation rates between groups.
```{python}
# Bivariate OLS regression: gave ~ treatment
model = smf.ols("gave ~ treatment", data=data).fit()
model.summary()
```
The results confirm what we observed in the t-test. The coefficient on the `treatment` variable is **0.0042**, with a **p-value of 0.002**, indicating that the difference in donation rates between treatment and control is statistically significant at the 1% level.

This suggests that receiving a matching donation offer increased the likelihood of giving by approximately **0.42 percentage points**. While this effect may seem small in absolute terms, it is consistent with the behavioral insights from Karlan and List (2007): even modest framing changes like mentioning a match can lead to meaningful increases in donor participation.

```{python}
# Probit regression: gave ~ treatment
probit_model = smf.probit("gave ~ treatment", data=data).fit()
probit_model.summary()
```

### Interpretation of Experimental Results

The evidence suggests that being assigned to the treatment group — receiving a matching donation offer — significantly increased the likelihood of making a charitable donation.

- The **bar plot** shows a noticeable difference in donation rates between the treatment and control groups.
- The **t-test** reveals a statistically significant difference in donation behavior (T = 3.209, p = 0.001), meaning the treatment group donated at a higher rate than the control group.
- The **OLS regression** confirms this finding. The treatment coefficient is **0.0042**, with a p-value of **0.002**, indicating that assignment to treatment increases the probability of donating by about 0.42 percentage points.
- The **Probit regression** also supports this: the coefficient on treatment is **0.0868** (p = 0.002), again showing a positive and statistically significant effect on the likelihood of giving.

Overall, these results replicate the findings in Karlan and List (2007): **matching donations increase the probability of giving**, even if the actual increase is modest in absolute terms. The consistency across all three statistical approaches (t-test, OLS, and probit) strengthens the causal interpretation of the results. This supports the idea that even small nudges like matching offers can influence donor behavior.




### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

### Differences between Match Rates

We explore whether larger match ratios (e.g., 2:1 or 3:1) result in higher likelihoods of donating compared to a 1:1 match.

---

### T-tests for Match Ratios

```{python}
# Filter only treatment group and each match ratio
ratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']
ratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']
ratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']

# T-tests
print("2:1 vs 1:1")
print(stats.ttest_ind(ratio2, ratio1, equal_var=False))

print("\n3:1 vs 2:1")
print(stats.ttest_ind(ratio3, ratio2, equal_var=False))
```


> **Interpretation**:  
> These t-tests compare donation rates for each match ratio within the treatment group.  
> - The **2:1 vs 1:1** test shows a [insert p-value here], and  
> - The **3:1 vs 2:1** test shows a [insert p-value here].  
>  
> In both comparisons, the p-values are relatively large, suggesting **no statistically significant difference** in donation likelihood across match sizes. This supports Karlan and List’s finding that **increasing the match ratio does not meaningfully improve the effectiveness of the match offer.**



---

### Regression on Match Ratio (Categorical)
```{python}
# Filter for treatment group and valid match ratios
match_data = data[(data['treatment'] == 1) & (data['ratio'].isin([1, 2, 3]))].copy()

# Properly set 'ratio' as a categorical variable with 1:1 as the baseline
match_data['ratio'] = pd.Categorical(match_data['ratio'], categories=[1, 2, 3])

# Run the regression: donation behavior by match ratio
model = smf.ols("gave ~ C(ratio)", data=match_data).fit()
model.summary()
```



> **Interpretation**:  
> The regression estimates donation rates for each match ratio using the 1:1 match as the baseline (intercept = 2.07%).  
> - The coefficient for `2:1` is **0.0019** (p = 0.338), and  
> - The coefficient for `3:1` is **0.0020** (p = 0.313).  
> 
> These coefficients are very small and **not statistically significant**, indicating that **higher match ratios do not meaningfully increase the likelihood of giving** beyond what is achieved with a 1:1 match.  
> 
> This supports the key finding in Karlan and List (2007): the **existence** of a match offer increases donations, but **increasing the ratio** (e.g., from 1:1 to 3:1) **does not further boost effectiveness**.


---


#### Response Rate Differences from Raw Data and Model

```{python}
# Group-level mean donation rates
ratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']
ratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']
ratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']

mean1 = ratio1.mean()
mean2 = ratio2.mean()
mean3 = ratio3.mean()

print(f"Response Rate 1:1 = {mean1:.4f}")
print(f"Response Rate 2:1 = {mean2:.4f}")
print(f"Response Rate 3:1 = {mean3:.4f}")

print(f"\n2:1 - 1:1 = {mean2 - mean1:.4f}")
print(f"3:1 - 2:1 = {mean3 - mean2:.4f}")
```

```{python}
# Model-based differences from regression coefficients
coef = model.params

print("\nModel-Based Differences:")
print(f"2:1 - 1:1 = {coef['C(ratio)[T.2]']:.4f}")
print(f"3:1 - 1:1 = {coef['C(ratio)[T.3]']:.4f}")
print(f"3:1 - 2:1 = {coef['C(ratio)[T.3]'] - coef['C(ratio)[T.2]']:.4f}")
```

> **Interpretation**:  
> Both the raw response rate differences and the model-based regression estimates suggest that increasing the match ratio from **1:1 to 2:1** leads to a very small increase in the probability of giving (approximately **0.0019**, or 0.19 percentage points).  
>
> Increasing the ratio further from **2:1 to 3:1** adds virtually no additional effect — a difference of just **0.0001**, or 0.01 percentage points. These differences are not only small in magnitude but also statistically insignificant based on earlier t-tests and regression outputs.  
>
> The fact that the **raw differences** and **regression coefficients** are nearly identical provides strong evidence that these small effects are consistent across estimation approaches.  
>
> In line with Karlan and List (2007), we conclude that **the presence of a match offer matters**, but **larger match ratios (like 3:1) do not provide additional benefit** over a standard 1:1 match. This supports their interpretation that psychological or motivational factors may be triggered by any match, not necessarily a more generous one.




---

### Summary

::: {.callout-note title="Conclusion"}
Neither the t-tests nor the regression provide strong evidence that increasing the match ratio from 1:1 to 2:1 or 3:1 significantly increases the donation response rate. The estimated differences are small and statistically insignificant. These findings replicate Karlan and List’s claim on page 8 that “larger match ratios do not provide additional lift,” and suggest that announcing any match offer may be sufficient to nudge donation behavior — but increasing the ratio adds little extra power.
:::



## Size of Charitable Contribution

In this subsection, I analyze the effect of the matching donation offer on the **amount donated**, not just whether individuals gave. I examine this in two ways: (1) using the full sample including non-donors, and (2) restricting the analysis to only those who made a donation.

---

### Full Sample Regression

The first model regresses the donation amount (`amount`) on the treatment assignment across the entire sample, including non-donors (who are coded as 0 for amount). This allows us to assess the **average treatment effect** on giving across all individuals.

```{python}
#| echo: false
model_full = smf.ols("amount ~ treatment", data=data).fit()
model_full.summary()
```

The regression results show that the **intercept** (average donation in the control group) is **$0.8133**, while the treatment effect is estimated at **$0.1536**. The p-value for the treatment coefficient is **0.063**, which is **marginally significant** at the 10% level.

This suggests that receiving a matching offer may lead to a small increase in the average donation amount across the full sample, but the evidence is not strong enough to be statistically significant at conventional 5% thresholds.

---

### Regression Among Donors Only

Next, I restrict the sample to individuals who actually made a donation (`gave == 1`) to test whether the matching offer affected **how much donors gave**, conditional on donating.

```{python}
#| echo: false
donors = data[data["gave"] == 1]
model_donors = smf.ols("amount ~ treatment", data=donors).fit()
model_donors.summary()
```

Among donors, the **average donation in the control group** is approximately **$45.54**, as reflected in the intercept. The treatment group gave on average **$1.67 less** than the control group, though this difference is **not statistically significant** (p = **0.561**).

---

### Interpretation

These results indicate that while the treatment may increase the **likelihood** of giving (as seen earlier), it does **not significantly affect the amount donated**. In the full sample, the treatment effect is small and only marginally significant. Among those who donated, the estimated effect is negative and clearly not statistically significant.

Because the treatment was randomly assigned, the coefficient estimates can be interpreted causally. However, it is important to note that the donor-only model is conditional on post-treatment behavior and may be affected by selection.

Overall, this analysis supports the main takeaway from Karlan and List (2007): **matching offers increase participation (extensive margin), but not the size of contributions (intensive margin)**.




#### Histogram of Donation Amounts: Treatment Group
```{python}
#| echo: false

sns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, color='darkorange')
plt.axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--', label='Mean')
plt.title("Donation Amounts – Treatment Group")
plt.xlabel("Amount")
plt.legend()
plt.show()
```



#### Histogram of Donation Amounts: Control Group

```{python}
#| echo: false
#| warning: false
#| message: false

sns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30)
plt.axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--', label='Mean')
plt.title("Donation Amounts – Control Group")
plt.xlabel("Amount")
plt.legend()
plt.show()
```


---

###  Interpretation

> In the regression using the full sample, we find that the treatment group donated $0.15 more on average than the control group. However, this result is not statistically significant (p = 0.063), meaning we cannot rule out that the difference is due to random chance.  
> 
> In the donor-only regression, we isolate just the people who made a donation and ask: do those in the treatment group give more than those in the control? Here, the treatment coefficient is -1.67, again not statistically significant (p = 0.561). In fact, the sign of the effect is slightly negative.
> 
> The histograms show that donation amounts are heavily skewed, with most donations under $100. The means for treatment and control are both around $45–46, with very similar distributions.  
> 
> Conclusion: The treatment (i.e., receiving a matching offer) appears to influence whether people give, but not how much they give. The full-sample regression can be interpreted causally because of random assignment, while the donor-only regression cannot, since it conditions on a post-treatment behavior (selection bias).  
> 
> Overall, our findings replicate Karlan and List (2007): matching gifts increase response rates, but do not significantly affect the donation size conditional on giving.




## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

We simulate two large samples:
- One from the control group (donation probability = 0.018)
- One from the treatment group (donation probability = 0.022)

We calculate the difference between matched pairs, and then compute the cumulative average difference.

### Law of Large Numbers

To illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability \( p = 0.018 \)), and one representing those who did (treatment, with \( p = 0.022 \)). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the **cumulative average** of these differences.

### Law of Large Numbers

To illustrate the Law of Large Numbers, I simulate two groups: one representing respondents who did not receive a matching donation offer (control, with donation probability \( p = 0.018 \)), and one representing those who did (treatment, with \( p = 0.022 \)). I draw 10,000 samples from each group and compute the difference for each pair. Then, I calculate and plot the **cumulative average** of these differences.

### Law of Large Numbers

To simulate the Law of Large Numbers, I compare paired random samples from two Bernoulli distributions: one for the control group (\( p = 0.018 \)) and one for the treatment group (\( p = 0.022 \)). I compute the difference in outcomes for each pair and plot the cumulative average to see whether it converges to the true mean difference of 0.004.

### Law of Large Numbers: Convergence of Mean Difference


```{python}
#| echo: false
#| warning: false
#| message: false

import numpy as np
import matplotlib.pyplot as plt

plt.style.use("classic")
np.random.seed(42)

n_obs = 6500
sample_size = 50

# Simulate cumulative difference in sample means
diffs = [
    np.random.binomial(1, 0.022, sample_size).mean() - 
    np.random.binomial(1, 0.018, sample_size).mean()
    for _ in range(n_obs)
]
cum_avg = np.cumsum(diffs) / np.arange(1, n_obs + 1)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(cum_avg, color='blue', linewidth=2.5, label="Cumulative Avg. Difference")
plt.axhline(y=0.004, color='red', linestyle='--', linewidth=2, label="True Mean Difference (0.004)")
plt.title("Law of Large Numbers: Convergence of Mean Difference", fontsize=13)
plt.xlabel("Number of Observations", fontsize=12)
plt.ylabel("Cumulative Difference (Treatment - Control)", fontsize=12)
plt.xlim(0, n_obs)
plt.ylim(0, 0.04)
plt.yticks(np.arange(0, 0.045, 0.005))
plt.gca().set_yticklabels([f"{tick:.3f}" for tick in np.arange(0, 0.045, 0.005)])  # <- fixes label format
plt.legend()
plt.grid(True)
plt.show()
```





As shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of **0.004**. This pattern exemplifies the **Law of Large Numbers**, which states that as the number of observations increases, the sample average converges to the population mean.

This visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.


As shown in the plot above, the cumulative average of the differences fluctuates at first but gradually stabilizes around the true expected difference of **0.004**. This pattern exemplifies the **Law of Large Numbers**, which states that as the number of observations increases, the sample average converges to the population mean.

This visualization demonstrates that even with a small true effect size, a sufficiently large number of observations can reveal consistent patterns. It provides a conceptual foundation for why the original Karlan and List experiment, which involved tens of thousands of letters, was able to detect subtle differences in donation behavior.




---

###  Interpretation
 
> The simulation demonstrates the **Law of Large Numbers**, which states that as the number of observations increases, the sample average converges to the population mean.  
> 
> In this plot, the cumulative average difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups begins with wide fluctuations due to randomness in the early observations. However, as we simulate more and more matched pairs (up to 100,000), the cumulative difference stabilizes and converges to the true expected difference of **0.004**.
>
> This reinforces a key idea in statistics: **with large enough sample sizes, the law of averages ensures reliable estimates of population parameters**. It also explains why Karlan and List were able to detect small treatment effects — because their experiment had tens of thousands of observations.
```



### Central Limit Theorem


We simulate distributions of average differences in donation rates between control (p = 0.018) and treatment (p = 0.022) groups, using increasing sample sizes (n = 50, 200, 500, 1000). For each sample size, we repeat the process 1000 times and visualize the distribution of sample differences.

```{python}
#| echo: false
#| warning: false
#| message: false

def run_clt_sim(n, sims=1000, p_control=0.018, p_treat=0.022):
    diffs = []
    for _ in range(sims):
        c = np.random.binomial(1, p_control, n)
        t = np.random.binomial(1, p_treat, n)
        diffs.append(np.mean(t) - np.mean(c))
    return diffs

# Sample sizes to simulate
sample_sizes = [50, 200, 500, 1000]
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for ax, n in zip(axes.flat, sample_sizes):
    dist = run_clt_sim(n)
    ax.hist(dist, bins=30, edgecolor='black')
    ax.set_title(f"Sample Size = {n}")
    ax.axvline(0, color='red', linestyle='--', label='Zero')
    ax.axvline(0.004, color='green', linestyle='--', label='True Difference')
    ax.legend()

plt.tight_layout()
plt.show()
```


---

###  Interpretation
 
> These four histograms illustrate the **Central Limit Theorem** using simulated sample means of donation rate differences between the treatment (p = 0.022) and control (p = 0.018) groups.  
> 
> As the sample size increases:
> - With **n = 50**, the distribution is wide and noisy — there's considerable variation, and the true mean difference (0.004) is not clearly distinguishable from zero.
> - At **n = 200**, the distribution begins to resemble a bell curve, and the true difference is more centered.
> - At **n = 500**, the histogram tightens noticeably, and zero begins to lie closer to the tail of the distribution.
> - By **n = 1000**, the sample mean differences are tightly clustered around 0.004, and **zero is clearly no longer at the center** — it lies in the tail, suggesting strong evidence against the null of no effect.
> 
> This simulation confirms that **larger samples reduce sampling variability** and make it easier to detect small, true differences — exactly what Karlan and List leveraged with their large-scale experiment.







