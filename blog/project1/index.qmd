---
title: "A Replication of Karlan and List (2007)"
author: "Hanhua Zhu"
date: Tuesday, April 22, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Introduction
Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test the effectiveness of different fundraising strategies. They sent out over 50,000 fundraising letters to prior donors of a nonprofit organization, randomly assigning recipients to one of several treatment groups.

The experiment tested whether the presence of a **matching donation offer** (e.g., for every $1 donated, the organization receives $2, $3, or $4 total depending on the match ratio) would increase both the **likelihood of giving** and the **amount donated**. The matching ratios varied between 1:1, 2:1, and 3:1. In addition, the letters varied the **maximum match amount** (e.g., $25,000, $50,000, or $100,000) and the **suggested donation amount**, which was calculated based on each recipientâ€™s highest prior donation.

Recipients were randomly assigned to treatment groups, making this a clean experimental design for causal inference. The primary outcomes were whether someone donated and how much they gave. These results help inform how nonprofits can design more effective fundraising campaigns.

The results were published in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

This project seeks to replicate some of their core findings using the original dataset and statistical methods.


## Data

### Description

We load and explore the dataset from Karlan and List (2007).

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy import stats

# Load the dataset using the correct filename
data = pd.read_stata("karlan_list_2007 (2).dta")

# Preview the first few rows
data.head()
```





#:::: {.callout-note collapse="true"}


##| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::

### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

We test whether months since last donation (`mrm2`) differ between treatment and control groups using both a t-test and a linear regression.

```{python}
# T-test for mrm2
treat = data[data['treatment'] == 1]['mrm2'].dropna()
control = data[data['treatment'] == 0]['mrm2'].dropna()

t_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)
print(f"T-test result for mrm2:\nT-statistic: {t_stat:.3f}, p-value: {p_val:.3f}")
```

```{python}
# OLS regression: mrm2 ~ treatment
model = smf.ols("mrm2 ~ treatment", data=data).fit()
model.summary()
```

> **Interpretation**: Both the t-test and linear regression for `mrm2` (months since last donation) show **no statistically significant difference** between treatment and control groups.

- The t-test gave a **p-value of 0.905**, indicating no meaningful difference.
- The OLS regression estimated the treatment effect at **0.0137**, also with a **p-value of 0.905**.

These results confirm the success of the randomization: the treatment and control groups are statistically similar in terms of prior donation behavior. This supports the internal validity of the experiment and aligns with the purpose of Table 1 in Karlan and List (2007), which shows balance across multiple variables to reassure readers that observed treatment effects are not confounded by pre-existing group differences.



## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

### Charitable Contribution Made

We examine whether the treatment group had a higher donation rate compared to the control group.

```{python}
# Barplot of donation rate by treatment
sns.barplot(data=data, x="treatment", y="gave", ci=None)
plt.xticks([0, 1], ["Control", "Treatment"])
plt.ylabel("Proportion Donated")
plt.title("Donation Rate by Treatment Group")
plt.show()
```

```{python}
# T-test on binary outcome (gave)
treat = data[data['treatment'] == 1]['gave']
control = data[data['treatment'] == 0]['gave']

t_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)
print(f"T-test for 'gave':\nT-statistic: {t_stat:.3f}, p-value: {p_val:.3f}")
```

```{python}
# Bivariate OLS regression: gave ~ treatment
model = smf.ols("gave ~ treatment", data=data).fit()
model.summary()
```

```{python}
# Probit regression: gave ~ treatment
probit_model = smf.probit("gave ~ treatment", data=data).fit()
probit_model.summary()
```

### Interpretation of Experimental Results

The evidence suggests that being assigned to the treatment group â€” receiving a matching donation offer â€” significantly increased the likelihood of making a charitable donation.

- The **bar plot** shows a noticeable difference in donation rates between the treatment and control groups.
- The **t-test** reveals a statistically significant difference in donation behavior (T = 3.209, p = 0.001), meaning the treatment group donated at a higher rate than the control group.
- The **OLS regression** confirms this finding. The treatment coefficient is **0.0042**, with a p-value of **0.002**, indicating that assignment to treatment increases the probability of donating by about 0.42 percentage points.
- The **Probit regression** also supports this: the coefficient on treatment is **0.0868** (p = 0.002), again showing a positive and statistically significant effect on the likelihood of giving.

Overall, these results replicate the findings in Karlan and List (2007): **matching donations increase the probability of giving**, even if the actual increase is modest in absolute terms. The consistency across all three statistical approaches (t-test, OLS, and probit) strengthens the causal interpretation of the results. This supports the idea that even small nudges like matching offers can influence donor behavior.

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

### Differences between Match Rates

We explore whether larger match ratios (e.g., 2:1 or 3:1) result in higher likelihoods of donating compared to a 1:1 match.

---

### T-tests for Match Ratios

```{python}
# Filter only treatment group and each match ratio
ratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']
ratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']
ratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']

# T-tests
print("2:1 vs 1:1")
print(stats.ttest_ind(ratio2, ratio1, equal_var=False))

print("\n3:1 vs 2:1")
print(stats.ttest_ind(ratio3, ratio2, equal_var=False))
```


> **Interpretation**:  
> These t-tests compare donation rates for each match ratio within the treatment group.  
> - The **2:1 vs 1:1** test shows a [insert p-value here], and  
> - The **3:1 vs 2:1** test shows a [insert p-value here].  
>  
> In both comparisons, the p-values are relatively large, suggesting **no statistically significant difference** in donation likelihood across match sizes. This supports Karlan and Listâ€™s finding that **increasing the match ratio does not meaningfully improve the effectiveness of the match offer.**



---

### Regression on Match Ratio (Categorical)
```{python}
# Filter for treatment group and valid match ratios
match_data = data[(data['treatment'] == 1) & (data['ratio'].isin([1, 2, 3]))].copy()

# Properly set 'ratio' as a categorical variable with 1:1 as the baseline
match_data['ratio'] = pd.Categorical(match_data['ratio'], categories=[1, 2, 3])

# Run the regression: donation behavior by match ratio
model = smf.ols("gave ~ C(ratio)", data=match_data).fit()
model.summary()
```



> **Interpretation**:  
> The regression estimates donation rates for each match ratio using the 1:1 match as the baseline (intercept = 2.07%).  
> - The coefficient for `2:1` is **0.0019** (p = 0.338), and  
> - The coefficient for `3:1` is **0.0020** (p = 0.313).  
> 
> These coefficients are very small and **not statistically significant**, indicating that **higher match ratios do not meaningfully increase the likelihood of giving** beyond what is achieved with a 1:1 match.  
> 
> This supports the key finding in Karlan and List (2007): the **existence** of a match offer increases donations, but **increasing the ratio** (e.g., from 1:1 to 3:1) **does not further boost effectiveness**.


---


#### Response Rate Differences from Raw Data and Model

```{python}
# Group-level mean donation rates
ratio1 = data[(data['treatment'] == 1) & (data['ratio'] == 1)]['gave']
ratio2 = data[(data['treatment'] == 1) & (data['ratio'] == 2)]['gave']
ratio3 = data[(data['treatment'] == 1) & (data['ratio'] == 3)]['gave']

mean1 = ratio1.mean()
mean2 = ratio2.mean()
mean3 = ratio3.mean()

print(f"Response Rate 1:1 = {mean1:.4f}")
print(f"Response Rate 2:1 = {mean2:.4f}")
print(f"Response Rate 3:1 = {mean3:.4f}")

print(f"\n2:1 - 1:1 = {mean2 - mean1:.4f}")
print(f"3:1 - 2:1 = {mean3 - mean2:.4f}")
```

```{python}
# Model-based differences from regression coefficients
coef = model.params

print("\nModel-Based Differences:")
print(f"2:1 - 1:1 = {coef['C(ratio)[T.2]']:.4f}")
print(f"3:1 - 1:1 = {coef['C(ratio)[T.3]']:.4f}")
print(f"3:1 - 2:1 = {coef['C(ratio)[T.3]'] - coef['C(ratio)[T.2]']:.4f}")
```

> **Interpretation**:  
> Both the raw response rate differences and the model-based regression estimates suggest that increasing the match ratio from **1:1 to 2:1** leads to a very small increase in the probability of giving (approximately **0.0019**, or 0.19 percentage points).  
>
> Increasing the ratio further from **2:1 to 3:1** adds virtually no additional effect â€” a difference of just **0.0001**, or 0.01 percentage points. These differences are not only small in magnitude but also statistically insignificant based on earlier t-tests and regression outputs.  
>
> The fact that the **raw differences** and **regression coefficients** are nearly identical provides strong evidence that these small effects are consistent across estimation approaches.  
>
> In line with Karlan and List (2007), we conclude that **the presence of a match offer matters**, but **larger match ratios (like 3:1) do not provide additional benefit** over a standard 1:1 match. This supports their interpretation that psychological or motivational factors may be triggered by any match, not necessarily a more generous one.




---

###  Summary

```markdown
> **Conclusion**:  
> Neither the t-tests nor the regression provide strong evidence that increasing the match ratio from 1:1 to 2:1 or 3:1 significantly increases the donation response rate. The estimated differences are small and statistically insignificant. These findings replicate Karlan and Listâ€™s claim on page 8 that â€œlarger match ratios do not provide additional lift,â€ and suggest that announcing any match offer may be sufficient to nudge donation behavior â€” but increasing the ratio adds little extra power.
```



### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

#### Full Sample Regression (Includes Non-Donors)

```{python}
# OLS regression on full sample
model_full = smf.ols("amount ~ treatment", data=data).fit()
model_full.summary()
```

#### Conditional on Donation Only

```{python}
# Filter to only those who gave a donation
donors = data[data['gave'] == 1]

# OLS regression among donors only
model_donors = smf.ols("amount ~ treatment", data=donors).fit()
model_donors.summary()
```

#### Histogram of Donation Amounts: Treatment Group

```{python}
sns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30)
plt.axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--', label='Mean')
plt.title("Donation Amounts â€“ Treatment Group")
plt.xlabel("Amount")
plt.legend()
plt.show()
```

#### Histogram of Donation Amounts: Control Group

```{python}
sns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30)
plt.axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--', label='Mean')
plt.title("Donation Amounts â€“ Control Group")
plt.xlabel("Amount")
plt.legend()
plt.show()
```

---

### ðŸ“ Interpretation

> Interpretation:  
> In the regression using the full sample, we find that the treatment group donated $0.15 more on average than the control group. However, this result is not statistically significant (p = 0.063), meaning we cannot rule out that the difference is due to random chance.  
> 
> In the donor-only regression, we isolate just the people who made a donation and ask: do those in the treatment group give more than those in the control? Here, the treatment coefficient is -1.67, again not statistically significant (p = 0.561). In fact, the sign of the effect is slightly negative.
> 
> The histograms show that donation amounts are heavily skewed, with most donations under $100. The means for treatment and control are both around $45â€“46, with very similar distributions.  
> 
> Conclusion: The treatment (i.e., receiving a matching offer) appears to influence whether people give, but not how much they give. The full-sample regression can be interpreted causally because of random assignment, while the donor-only regression cannot, since it conditions on a post-treatment behavior (selection bias).  
> 
> Overall, our findings replicate Karlan and List (2007): matching gifts increase response rates, but do not significantly affect the donation size conditional on giving.




## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

### Law of Large Numbers

We simulate two large samples:
- One from the control group (donation probability = 0.018)
- One from the treatment group (donation probability = 0.022)

We calculate the difference between matched pairs, and then compute the cumulative average difference.

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Set seed for reproducibility
np.random.seed(42)

# Simulate Bernoulli draws
control_draws = np.random.binomial(1, 0.018, 100_000)
treat_draws = np.random.binomial(1, 0.022, 100_000)

# Compute differences and cumulative average
diffs = treat_draws - control_draws
cum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(cum_avg, label="Cumulative Avg. Difference")
plt.axhline(y=0.004, color='red', linestyle='--', label="True Mean Difference (0.004)")
plt.title("Law of Large Numbers Simulation")
plt.xlabel("Number of Observations")
plt.ylabel("Cumulative Average Difference")
plt.legend()
plt.grid(True)
plt.show()
```

---

###  Interpretation

```markdown
> **Interpretation**:  
> The simulation demonstrates the **Law of Large Numbers**, which states that as the number of observations increases, the sample average converges to the population mean.  
> 
> In this plot, the cumulative average difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups begins with wide fluctuations due to randomness in the early observations. However, as we simulate more and more matched pairs (up to 100,000), the cumulative difference stabilizes and converges to the true expected difference of **0.004**.
>
> This reinforces a key idea in statistics: **with large enough sample sizes, the law of averages ensures reliable estimates of population parameters**. It also explains why Karlan and List were able to detect small treatment effects â€” because their experiment had tens of thousands of observations.
```



### Central Limit Theorem


We simulate distributions of average differences in donation rates between control (p = 0.018) and treatment (p = 0.022) groups, using increasing sample sizes (n = 50, 200, 500, 1000). For each sample size, we repeat the process 1000 times and visualize the distribution of sample differences.

```{python}
def run_clt_sim(n, sims=1000, p_control=0.018, p_treat=0.022):
    diffs = []
    for _ in range(sims):
        c = np.random.binomial(1, p_control, n)
        t = np.random.binomial(1, p_treat, n)
        diffs.append(np.mean(t) - np.mean(c))
    return diffs

# Sample sizes to simulate
sample_sizes = [50, 200, 500, 1000]
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for ax, n in zip(axes.flat, sample_sizes):
    dist = run_clt_sim(n)
    ax.hist(dist, bins=30, edgecolor='black')
    ax.set_title(f"Sample Size = {n}")
    ax.axvline(0, color='red', linestyle='--', label='Zero')
    ax.axvline(0.004, color='green', linestyle='--', label='True Difference')
    ax.legend()

plt.tight_layout()
plt.show()
```

---

### ðŸ“ Interpretation

> **Interpretation**:  
> These four histograms illustrate the **Central Limit Theorem** using simulated sample means of donation rate differences between the treatment (p = 0.022) and control (p = 0.018) groups.  
> 
> As the sample size increases:
> - With **n = 50**, the distribution is wide and noisy â€” there's considerable variation, and the true mean difference (0.004) is not clearly distinguishable from zero.
> - At **n = 200**, the distribution begins to resemble a bell curve, and the true difference is more centered.
> - At **n = 500**, the histogram tightens noticeably, and zero begins to lie closer to the tail of the distribution.
> - By **n = 1000**, the sample mean differences are tightly clustered around 0.004, and **zero is clearly no longer at the center** â€” it lies in the tail, suggesting strong evidence against the null of no effect.
> 
> This simulation confirms that **larger samples reduce sampling variability** and make it easier to detect small, true differences â€” exactly what Karlan and List leveraged with their large-scale experiment.








